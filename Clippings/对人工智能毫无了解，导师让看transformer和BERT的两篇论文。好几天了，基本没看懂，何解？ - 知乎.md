---
title: "(93 封私信 / 2 条消息) 对人工智能毫无了解，导师让看transformer和BERT的两篇论文。好几天了，基本没看懂，何解？ - 知乎"
source: "https://www.zhihu.com/question/568969384/answer/129407278073"
author:
created: 2025-03-25
description: "如何在导师的\"关爱\"下，用三天速成Transformer和BERT，顺便把\"看不懂论文\"包装成\"战…"
tags:
  - "clippings"
---
[人工智能](https://www.zhihu.com/topic/19551275) [深度学习（Deep Learning）](https://www.zhihu.com/topic/19813032) [BERT](https://www.zhihu.com/topic/20743626)

## 对人工智能毫无了解，导师让看transformer和BERT的两篇论文。好几天了，基本没看懂，何解？

如题 [查看全部 79 个回答](https://www.zhihu.com/question/568969384)

如何在导师的"关爱"下，用三天速成Transformer和BERT，顺便把"看不懂论文"包装成"战略性思考"。

一、Transformer：前浪们的"骨灰级葬礼"

首先，请允许我向所有RNN和CNN的遗老遗少们默哀三秒。2017年，谷歌八壮士（江湖人称Transformer创始八子）用一篇论文，把循环神经网络和卷积神经网络拍在了沙滩上，还顺手点了个"踩"。

这帮狠人搞了个骚操作：注意力就是生产力。传统模型像极了在食堂排队打饭的你——必须按顺序处理每个词，而Transformer直接开启上帝视角，瞬间扫描全文所有字词关系。这就好比让社畜同时处理20个微信工作群，还要求每个群都回复"收到"，而Transformer直接开启自动回复外挂，把"并行计算"四个字刻进了DNA。

更骚的是 [多头注意力机制](https://zhida.zhihu.com/search?content_id=719257443&content_type=Answer&match_order=1&q=%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&zhida_source=entity) ，相当于让8个不同部门的HR同时给同一个简历打分：有人看学历，有人看颜值，有人看能不能接受007。最后综合评判，把打工人精准分类到合适岗位。这套组合拳直接让机器翻译任务BLEU值暴涨2分，相当于你老板突然给你涨薪200%，但实际只加了200块。

二、BERT：谷歌的"职场PUA大师"

如果说Transformer是屠龙刀，那2018年的BERT就是倚天剑。谷歌的码农们深谙职场厚黑学，开发出两大训练绝技：

1\. [MLM](https://zhida.zhihu.com/search?content_id=719257443&content_type=Answer&match_order=1&q=MLM&zhida_source=entity) （完形填空）：随机遮住15%的单词，让AI在"老板今天说要\_\_（画饼/加薪）"的语境中修炼。更损的是，10%的单词会被替换成脏数据，完美模拟职场中同事传话时添油加醋的场景。

2\. [NSP](https://zhida.zhihu.com/search?content_id=719257443&content_type=Answer&match_order=1&q=NSP&zhida_source=entity) （鉴婊达人）：训练AI识别两句话是不是原配。比如"老板说上市后全员分红"接"三年后公司破产清算"，这种堪比《甄嬛传》的塑料姐妹情，BERT分分钟鉴出是野鸡CP。

这套组合拳直接把AI调教成了办公室政治高手，在11项NLP任务中拳打GPT，脚踢 [ELMo](https://zhida.zhihu.com/search?content_id=719257443&content_type=Answer&match_order=1&q=ELMo&zhida_source=entity) ，堪称算法界的"卷王之王"。不过代价是340M参数的庞大体量，相当于让打工人背着房贷健身，练出肌肉的同时也练出了高血压。

三、学术民工的自我救赎

看不懂论文怎么办？记住三大生存法则：

1\. 黑话翻译学：

"多头注意力"=找八个备胎同时分析

"位置编码"=给每个字发工牌防止摸鱼

"残差连接"=允许方案打补丁，KPI不够PPT来凑

2\. 装逼三件套：

在组会上抛出："我认为MLM的遮蔽策略存在表征偏差"，然后火速转移话题到食堂新菜。当导师眼睛发亮时，立即补充："当然这只是抛砖引玉，还要听老师指导"

3\. 战略性提问：

永远别问"这是什么意思"，要问："这个设计与xxx论文的差异是否源于梯度消失问题的不同解决方案？"。导师绝对想不到，你连梯度消失是啥都不知道。

最后友情提示：当代AI论文本质是大型传销现场。当你看到"模型参数量提升100倍，准确率增加2%"时，请自动脑补成"程序员又骗到算力经费了"。记住，真正的大师从不看论文——他们直接让研究生看。

[发布于 2025-03-22 07:43](https://www.zhihu.com/question/568969384/answer/129407278073) ・IP 属地菲律宾还没有人送礼物，鼓励一下作者吧

### 继续追问

由知乎直答提供[

Transformer如何实现并行计算？

](https://zhida.zhihu.com/search?content_id=719257443&content_type=ANSWER&is_preview=1&q=Transformer%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%EF%BC%9F&zhida_source=below_banner_question)[

BERT用了哪些训练绝技？

](https://zhida.zhihu.com/search?content_id=719257443&content_type=ANSWER&is_preview=1&q=BERT%E7%94%A8%E4%BA%86%E5%93%AA%E4%BA%9B%E8%AE%AD%E7%BB%83%E7%BB%9D%E6%8A%80%EF%BC%9F&zhida_source=below_banner_question)[

MLM训练中为什么要遮蔽单词？

](https://zhida.zhihu.com/search?content_id=719257443&content_type=ANSWER&is_preview=1&q=MLM%E8%AE%AD%E7%BB%83%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%81%AE%E8%94%BD%E5%8D%95%E8%AF%8D%EF%BC%9F&zhida_source=below_banner_question)

#### 更多回答

在没有基础的情况下直接读Transformer和BERT，肯定是读不懂的。再详尽通俗的解释也没法在毫无基础的情况下讲清楚Transformer。这就像让小学生直接学习求解方程，如果没有掌握基本的四则运算，不理解为什么要设未知数，是根本无法学习解方程的。学习还是要循序渐进，跳过音阶和练习曲直接来大作品，大概天才音乐家才能做到。

好的学习路线是追本溯源，打牢基础，不要死磕这两篇论文，看看他们引用的参考文献，看看前序工作，更加有助于理解Transformer和BERT。对于Transformer来说，学习路径应该是“基于MLP神经语言模型”，“词嵌入(Word2Vec)”，"基于RNN的序列到序列模型(Seq2Seq)"，“注意力机制”，然后Transformer和BERT就完全是水到渠成了。基础知识是简单的多层前馈神经网络（多层感知机MLP）和循环神经网络（RNN/LSTM）。不要以为Transformer替代了RNN就可以跳过他，跳过RNN，大概是无法理解为什么要引入注意力机制，到底解决了什么问题的。

**第一篇文章，应该从图灵奖得主Bengio 2003年的神经语言模型NNLM开始** ： [bengio03a.dvi](https://link.zhihu.com/?target=https%3A//jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) 。从这篇文章了解语言模型的基本任务：预测下一个词，看最简单的多层前馈神经网络如何学习语言模型，看到“词元”如何变成数值向量进入神经网络计算过程。

**第二篇文章，词嵌入Word2Vec原作(2013)** ： [\[1301.3781\] Efficient Estimation of Word Representations in Vector Space](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1301.3781) 。这篇文章其实在模型上没有太多值得学习的创新，模型依然很简单，任务也很简单，但是Word2Vec已经开始解释语言模型学习的本质，把语言序列嵌入到高维向量空间，从词嵌入开始，BERT实际上就是做到了句子嵌入。学习这种嵌入，不需要特殊任务，就是通过学习语言模型，预测下一个词，或者做填空任务。这篇文章揭示了学习语言模型得到的词嵌入在语义空间中自然呈现的规律性和可线性计算特性。这一部分可以参考阅读作者的另外两篇文章： [Linguistic Regularities in Continuous Space Word Representations](https://link.zhihu.com/?target=https%3A//aclanthology.org/N13-1090.pdf) ， [\[1310.4546\] Distributed Representations of Words and Phrases and their Compositionality](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1310.4546) 。这两篇文章详细揭示了语义空间中的规律性(linguistic regularity)，并把这种规律从词嵌入拓展到了词组嵌入。

这张图来自Word2Vec的相关论文，反复被用来展示语义空间中嵌入的规律性

**第三篇文章，序列到序列模型（Seq2Seq,2014）** [\[1409.3215\] Sequence to Sequence Learning with Neural Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.3215) ，作者 **Sutskever** 后来在OpenAI参与主导了GPT系列模型的工作。这篇文章是他在谷歌时期的有关机器翻译的工作，这个模型相当长一段时间占据了NLP的主流地位，乃至在Transformer诞生之后的相当一段时间里，编码器和解码器的思想仍然是NLP中很重要的建模思路。机器翻译，就是把源语言通过编码器映射到语义空间，再通过解码器从语义空间映射到目标语言。编码器换成图像编码器，就可以用于图像标签生成；解码器换成图像生成模型，就可以用于文生图。Seq2Seq的问题在于把所有语义信息映射到语义空间中的一个点，这个点成了瓶颈，容量有限容易损失信息，长期依赖记忆能力较差，这是注意力机制出现的契机。

**第四篇文章，机器翻译中的注意力机制（2015）** ， [\[1409.0473\] Neural Machine Translation by Jointly Learning to Align and Translate](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.0473) ，这还是Bengio指导的工作，发表于2015年，在机器翻译的工作中引入了注意力机制，模型的范式是编码器-解码器结构，采用RNN实现，Bengio这个模型用到RNN是GRU模型，前面Sutskever用的是LSTM。Bengio等人引入了注意力机制，实现了目标语言和源语言的软对齐，注意力就像是指针指向输入内容（源语言文本），跳开了映射到语义空间中一个点的限制，可以指向若干个点，根据解码的需要随时“调阅”输入内容。这就是注意力机制的核心作用。

注意力最初的计算方式（某种软对齐结果的加权求和）

注意力的最初计算方式是将“软对齐”结果加权求和，“软对齐”的方式采用向量内积（点积），就是Transformer中经典的缩放点积注意力。Tranformer加了一层线性变换，把“对齐”的内容(Query/Key)和“求和”的内容(Value)拆分成Query/Key/Value，增加了多重对齐机制，也就是“多头注意力”。将RNN的计算并行化，因此舍弃了RNN蕴含的序列前后关系信息，引入了额外的位置编码。  

注意力机制在机器翻译中实现了从源语言到目标语言的软对齐

看到这里，再去读Transformer和BERT，应该毫无压力了，一切都是水到渠成。  
  
插播广告：推荐一本书，适合从头入门机器学习，数学原理搭配丰富的Python代码实践： [《机器学习入门：数学原理解析及算法实践》(董政)【摘要 书评 试读】- 京东图书 (jd.com)](https://link.zhihu.com/?target=https%3A//item.jd.com/13733932.html)

**Transformer** ，非常推荐 [@李沐](https://www.zhihu.com/people/13fd0fce2affd948bfd821a8f7ed10f3) 老师和各位大佬的dive2dl里面的Transformer部分。

这个章节从最初的Attention开始，然后到Self-Attention，Multi-Head Attention等，到Transformer，以及随后的应用（基于Transformer的Pretraining，比如BERT）。相对会更容易理解一些，包括算法设计的intuition。

也有中文版 [《动手学深度学习》 - 动手学深度学习 2.0.0-beta1 documentation](https://link.zhihu.com/?target=https%3A//zh.d2l.ai/)

  

**BERT** 的论文原文写得确实不是特别容易懂，推荐可以参考Illustrated BERT：

这个博客里面讲解得相对比较清楚，图文并茂。

[查看全部 79 个回答](https://www.zhihu.com/question/568969384)
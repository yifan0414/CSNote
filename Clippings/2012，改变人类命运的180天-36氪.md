---
title: "2012，改变人类命运的180天-36氪"
source: "https://36kr.com/p/2421889040802823"
author:
published: 2025-01-02
created: 2025-01-02
description: "当人工智能的第一个音符奏响"
tags:
  - "clippings"
---
2012年12月初的一天，一场秘密竞拍正在美国滑雪胜地太浩湖（Lake Tahoe）的一家赌场酒店里进行。

太浩湖位于加州和内华达州交界处，是北美最大的高山湖泊，拥有蓝宝石般的湖面和顶级雪道，《教父2》曾在这里取景，马克吐温曾在此地流连忘返，而由于离旧金山湾区只有200多英里，这里常被称为“硅谷后花园”，扎克伯格和拉里·埃里森等大佬也在此圈地占山，兴建豪宅。

秘密竞拍的对象，是一家刚刚成立1个月、仅有3名员工的公司——DNNresearch，创立者是多伦多大学教授杰夫·辛顿（Geoffrey Hinton）和他两名学生。

这家公司没有任何有形的产品或资产，但追求者的身份暗示出了它的分量——**四位买家分别是Google、微软、DeepMind和百度。**

![](https://img.36krcdn.com/hsossms/20230907/v2_04deed7c3d12490881beb6ee2021e0bc@000000_oswg49029oswg702oswg352_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

举行秘密竞拍的Harrah's酒店，太浩湖，2012年

65岁的辛顿苍老，瘦削，饱受腰椎间盘的疼痛，他坐在酒店703房间的地板上为竞拍设置规则——起价1200万美元，抬价单位至少100万美元。

几个小时后，竞拍者就把价格推到了4400万美元，辛顿有些头晕，“感觉我们像是在拍电影”，于是果断喊停，并决定把公司卖给最后的喊价者——Google。

有意思的是，这场4400万美元竞拍的源头之一，就是来自于6个月前的Google。

2012年6月，Google研究部门Google Brain公开了The Cat Neurons项目（即“谷歌猫”）的研究成果。这个项目简单说就是用算法在YouTube的视频里识别猫，它由从斯坦福跳槽来Google的吴恩达发起，拉上了Google传奇人物Jeff Dean入伙，还从Google创始人Larry Page那里要到了大笔的预算。

谷歌猫项目搭建了一个神经网络，从YouTube上下载了大量的视频，不做标记，让模型自己观察和学习猫的特征，然后动用了遍布Google各个数据中心的16000个CPU来进行训练（内部以过于复杂和成本高为由拒绝使用GPU），最终实现74.8%的识别准确率。这一数字震惊业界。

吴恩达在“谷歌猫”项目临近结束前激流勇退，投身自己的互联网教育项目，临走前他向公司推荐了辛顿来接替他的工作。面对邀请，辛顿表示自己不会离开大学，只愿意去Google“待一个夏天”。由于Google招聘规则的特殊性，时年64岁的辛顿成为了Google历史上最年长的暑期实习生。

辛顿从80年代开始就战斗在人工智能的最前线，作为教授更是桃李满门（包括吴恩达），是深度学习领域的宗师级人物。因此，当他了解了“谷歌猫”项目的技术细节后，他马上就看到了项目成功背后的隐藏缺陷：“他们运行了错误的神经网络，并使用了错误的计算能力。”

同样的任务，辛顿认为自己可以做的更好。于是在短暂的“实习期”结束后，他马上投入行动。

辛顿找来了自己的两个学生——Ilya Sutskever和Alex Krizhevsky，两人都是出生于苏联的犹太人，前者极具数学天赋，后者擅长工程实现，三人密切配合后创建了一个新神经网络，然后马上参加了ImageNet图像识别比赛（ILSVRC），最后以惊人的84%识别准确率夺得冠军 。

2012年10月，辛顿团队在佛罗伦萨举行的计算机视觉会议上介绍了冠军算法AlexNet，相比谷歌猫用了16000颗CPU，AlexNet只用了4颗英伟达GPU，学术界和产业界彻底轰动，AlexNet的论文成为计算机科学史上最有影响力的论文之一，目前被引次数已经超过12万，而谷歌猫则被迅速遗忘。

![](https://img.36krcdn.com/hsossms/20230907/v2_311edd3c25174e6bb35973b901b5f42f@000000_oswg51051oswg1064oswg798_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

DNNresearch公司三人组

曾拿过第一届ImageNet大赛冠军的余凯读完论文后异常兴奋，“像触电了一样”。余凯是一名出生于江西的深度学习专家，刚从NEC跳去百度，他马上给辛顿写邮件，表达了合作的想法，辛顿欣然同意，并索性把自己和两名学生打包成一家公司，邀请买家竞拍，于是便有了开头的那一幕。

拍卖落槌后，一场更大的竞赛展开了：Google乘胜追击，2014年又把DeepMind收入囊中，“天下英雄尽入彀中”；而DeepMind则在2016年推出了AlphaGo，震惊全球；输给Google的百度则下定决心押注AI，十年投入千亿，余凯后来帮百度请来了吴恩达，他自己则在几年后离职创办了地平线。

微软表面上看慢了一拍，但最终却赢下了最大的战利品——OpenAI，后者的创始人就包括辛顿两个学生之一的Ilya Sutskever。而辛顿自己则一直在Google待到2023年，期间荣获ACM图灵奖。当然，跟Google的4400万美元比（辛顿分得40%），图灵奖的100万美元奖金就显得像是零花钱了。

从6月的谷歌猫，到10月的AlexNet论文，再到12月的太浩湖竞拍，差不多6个月的时间里，AI浪潮的伏笔几乎被全部埋下——深度学习的繁荣、GPU和英伟达的崛起、AlphaGo的称霸、Transformer的诞生、ChatGPT的横空出世……硅基盛世的宏大乐章奏响了第一个音符。

2012年从6月到12月的180天，碳基人类的命运被永远改变了——只有极少人意识到了这一点。

## **液体猫**

在这些极少数人中，斯坦福大学教授李飞飞是其中之一。

2012年，当辛顿参加ImageNet比赛结果出炉时，刚生完孩子的李飞飞还在休产假，但辛顿团队的错误率让她意识到历史正在被改写。作为ImageNet挑战赛的创办者，她买了当天最后一班飞机飞往佛罗伦萨，亲自为辛顿团队颁奖\[2\]。

李飞飞出生于北京，在成都长大，16岁时随父母移民美国，一边在洗衣店帮忙，一边读完普林斯顿。2009年李飞飞进入斯坦福担任助理教授，研究方向是计算机视觉与机器学习，这个学科的目标是让计算机能够像人一样，自己理解图片和影像的含义。

比如，当照相机拍下一只猫时，它只是通过传感器把光线转化成了像素，并不知道镜头里的东西是猫还是狗。如果把照相机比做人类的眼睛，计算机视觉解决的问题就是给照相机装上人的大脑。

传统的方式是将现实世界中的事物抽象为数学模型，比如将猫的特征抽象为简单的几何图形，就能大幅度降低机器识别的难度。

![](https://img.36krcdn.com/hsossms/20230907/v2_4159e5e886d64014ba89ae9ddf5c2f8b@000000_oswg209873oswg1080oswg445_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

图片来源：李飞飞的TED演讲

但这种思路有非常大的局限性，因为猫很有可能是这样的：

![](https://img.36krcdn.com/hsossms/20230907/v2_3967050d5974487a9c4ff8cad01721a9@000000_oswg768286oswg1000oswg500_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

为了让计算机能够识别“液体猫”，杰夫·辛顿、杨立昆（Yann LeCun）等一大批深度学习先驱从80年代就开始了探索。但总会撞到算力或算法的瓶颈——好的算法缺少足够的算力驱动，算力需求小的算法难以满足识别精度，无法产业化。

如果解决不了“液体猫”的问题，深度学习的性感就只能停留在理论层面，自动驾驶、医疗影像、精准广告推送这些产业化场景就只是空中楼阁。

简单来说，深度学习的发展需要**算法、算力、数据**三驾马车来拉动，**算法**决定了计算机用什么方式识别事物；但算法又需要足够大的**算力**来驱动；同时，算法的提升又需要大规模高质量的**数据**；三者相辅相成，缺一不可。

2000年后，尽管算力瓶颈伴随芯片处理能力的突飞猛进逐步消除，但主流学界对深度学习路线仍旧兴趣寡然。李飞飞意识到，瓶颈可能不是算法本身的精度，而在于缺乏高质量、大规模的数据集。

李飞飞的启发来自三岁孩子认识这个世界的方式——以猫为例，孩子会在大人的教导下一次又一次遇见猫，逐渐掌握猫的含义。如果把孩子的眼睛当作照相机，眼球转动一次等于按一次快门，那么，一个三岁的孩子就已经拍摄了上亿张照片。

把这个方法套在计算机上，假如给计算机不停的看包含猫和其他动物的图片，同时在每张图片背后写下正确答案。计算机每看一次图片，就和背面的答案核对一次。那么只要次数够多，计算机就有可能像孩子一样掌握猫的含义。

唯一需要解决的问题就是：**上哪找那么多写好答案的图片？**

![]( https://img.36krcdn.com/hsossms/20230907/v2_9e4c394877d846a2adf30bb494eac534@000000_oswg521472oswg1080oswg810_img_000?x-oss-process=image/format ,jpg/interlace,1/format,jpg/interlace,1)

李飞飞在2016年来到中国，宣布谷歌AI中国中心成立

这就是ImageNet诞生的契机。当时，即便是最大规模的数据集PASCAL，也只有四个类别总共1578张图片，而李飞飞的目标是创建一个包含几百个类别总共上千万张的数据集。现在听起来似乎不难，但要知道那是2006年，全球最流行的手机还是诺基亚5300。

依靠亚马逊众包平台，李飞飞团队解决了人工标注的庞大工作量。2009年，包含320万张图片的ImageNet数据集诞生。有了图片数据集，就可以在此基础上训练算法，让计算机提升识别能力。但相比三岁孩子的上亿张照片，320万的规模还是太少了。

为了让数据集不断扩充，李飞飞决定效仿业内流行的做法，举办图片识别大赛，参赛者自带算法识别数据集中的图片，准确率最高者获胜。但深度学习路线在当时并不是主流，ImageNet一开始只能“挂靠”在欧洲知名赛事PASCAL下面，才能勉强凑够参赛人数。

到了2012年，ImageNet的图片数量扩大到了1000个类别总共1500万张，李飞飞用6年时间补足了数据这块短板。不过，ILSVRC的最好成绩错误率也有25%，在算法和算力上，依然没有表现出足够的说服力。

这时，辛顿老师带着AlexNet和两块GTX580显卡登场了。

## **卷积**

辛顿团队的冠军算法AlexNet，采用了一种名叫卷积神经网络（Convolutional Neural Networks，简称CNN）的算法。“神经网络”在人工智能领域是个极其高频的词汇，也是机器学习的一个分支，其名称和结构都取材自人脑的运作方式。

人类辨识物体的过程是瞳孔先摄入像素，大脑皮层通过边缘和方位做初步处理，然后大脑通过不断的抽象来判定。因此，人脑可以根据一些特征就能判别出物体。

比如不用展示整张脸，大部分人都能认出下图中的人是谁：

![](https://img.36krcdn.com/hsossms/20230907/v2_5a32088bbbad4ddeafcc1de40bf400ba@000000_oswg390000oswg1064oswg384_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

神经网络其实就是模拟人脑的识别机制，理论上人脑能够实现的智能计算机也能实现。相较SVM、决策树、随机森林等方法，只有模拟人脑，才能处理类似“液体猫”和“半个特朗普”这种非结构化数据。

但问题是，人脑约有1000亿个神经元，神经元之间的节点（也就是突触）更是多达万亿，组成了一个无比复杂的网络。作为对比，用了16000个CPU组成的“谷歌猫”，内部共有10亿个节点，而这已经是当时最复杂的计算机系统了。

这也是为什么连“人工智能之父”Marvin Minsky都不看好这条路线，在2007年出版新书《The Emotion Machine》时，Minsky依然表达了对神经网络的悲观。为了改变主流机器学习界对人工神经网络的长期的消极态度，辛顿干脆将其改名为深度学习（Deep Learning）。

2006年，辛顿在Science上发表了一篇论文，提出了“深度信念神经网络（DBNN）”的概念，给出了一种多层深度神经网络的训练方法，被认为是深度学习的重大突破。但辛顿的方法需要消耗大量的算力和数据，实际应用难以实现。

深度学习需要不停的给算法喂数据，当时的数据集规模都太小了，直到ImageNet出现。

ImageNet的前两届比赛里，参赛团队使用了其他的机器学习路线，结果都相当平庸。而辛顿团队在2012年采用的卷积神经网络AlexNet，改良自另一位深度学习先驱杨立昆（Yann LeCun），其在1998年提出的LeNet让算法可以提取图像的关键特征，比如特朗普的金发。

同时，卷积核会在输入图像上滑动，所以无论被检测物体在哪个位置，都能被检测到相同的特征，大大减少了运算量。

AlexNet在经典的卷积神经网络结构基础上，摒弃了此前的逐层无监督方法，对输入值进行有监督学习，大大提高了准确率。

比如下图中右下角的图片，AlexNet其实并没有识别出正确答案（马达加斯加猫），但它列出的都是和马达加斯加猫一样会爬树的小型哺乳动物，这意味着算法不仅可以识别对象本身，还可以根据其他物体进行推测\[5\]。

![](https://img.36krcdn.com/hsossms/20230907/v2_7e7ee391ed7c423ab24356bf27611e75@000000_oswg1069456oswg980oswg798_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

图片来源：AlexNet论文

而产业界感到振奋的是，AlexNet有6000万个参数和65万个神经元，完整训练ImageNet数据集至少需要262千万亿次浮点运算。但辛顿团队在一个星期的训练过程中，只用了两块英伟达GTX 580显卡。

## **GPU**

辛顿团队拿到冠军后，最尴尬的显然是Google。

据说Google在内部也做了ImageNet数据集的测试，但识别精度远远落后于辛顿团队。考虑到Google拥有业界无法企及的硬件资源，以及搜索和YouTube的庞大数据规模，Google Brain更是领导钦点特事特办，其结果显然不具备足够的说服力。

如果没有这种巨大的反差，深度学习可能也不会在短时间内震撼业界，得到认可和普及。产业界感到振奋的原因在于辛顿团队只用了四块GPU，就能达到这么好的效果，那么算力就不再是瓶颈。

算法在训练时，会对神经网络每层的函数和参数进行分层运算，得到输出结果，而GPU恰好有非常强的并行运算能力。吴恩达在2009年的一篇论文中其实证明了这一点，但在和Jeff Dean运行“谷歌猫”时，他们还是用了CPU。后来Jeff Dean专门订购了200万美元的设备，依然不包括GPU\[6\]。

辛顿是极少数很早就意识到GPU之于深度学习巨大价值的人，然而在AlexNet刷榜之前，高科技公司普遍对GPU态度不明。

2009年，辛顿曾受邀去微软做一个语音识别项目的短期技术顾问，他建议项目负责人邓力购买最顶级的英伟达GPU，还要搭配对应的服务器。这个想法得到了邓力的支持，但邓力的上司Alex Acero认为这纯属乱花钱\[6\]，“GPU是用来玩游戏的，而不是用来做人工智能研究的。”

![](https://img.36krcdn.com/hsossms/20230907/v2_f4540ef49eac40b2b1b61d040119cd68@000000_oswg720798oswg1040oswg632_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

邓力

有趣的是，Alex Acero后来跳槽去了苹果，负责苹果的语音识别软件Siri。

而微软对GPU的不置可否显然让辛顿有些火大，他后来在一封邮件里建议邓力购买一套设备，而自己则会买三套，并且阴阳怪气的说\[6\]：毕竟我们是一所财力雄厚的加拿大大学，不是一家资金紧张的软件销售商。

但在2012年ImageNet挑战赛结束后，所有人工智能学者和科技公司都对GPU来了个180度大转弯。2014年，Google的GoogLeNet以93%的识别准确率夺冠，采用的正是英伟达GPU，这一年，所有参赛团队GPU的使用数量飙升到了110块。

这届挑战赛之所以被视为“大爆炸时刻”，在于深度学习的三驾马车——算法、算力、数据上的短板都被补足，产业化只剩下了时间问题。

![](https://img.36krcdn.com/hsossms/20230907/v2_e72f9968790b485995e3c698d2dac29b@000000_oswg83414oswg1080oswg863_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

**算法层面，辛顿团队发表的关于AlexNet的论文，成为计算机科学领域被引用次数最多的论文之一。原本百家争鸣的技术路线成了深度学习一家独大，几乎所有的计算机视觉研究都转向了神经网络。**

**算力层面，GPU超强并行计算能力与深度学习的适应性迅速被业界认可，六年前开始布局CUDA的英伟达成为了最大的赢家。**

**数据层面，ImageNet成为图像处理算法的试金石，有了高质量的数据集，算法识别精度日行千里。2017年最后一届挑战赛，冠军算法的识别准确率达到97.3%，超过了人类。**

2012年10月底，辛顿的学生Alex Krizhevsky在意大利佛罗伦萨的计算机视觉会议上公布了论文。然后，全世界的高科技公司开始不计成本地两件事：**一是买光英伟达的显卡，二是挖光大学里的AI研究员。**

太浩湖的4400万美元，给全球的深度学习大神做了一次重新定价。

## **夺旗**

从公开可查的信息看，当时还在百度的余凯的确是第一个来挖辛顿的人。

当时，余凯在百度担任百度多媒体部的负责人，也就是百度深度学习研究院（IDL）的前身。在收到余凯的邮件后，辛顿很快就回复说同意合作，顺带提出了希望百度提供一些经费的愿望。余凯问具体数字，辛顿表示100万美元就够——这个数字低到令人难以置信，只能雇两个P8。

余凯向李彦宏请示，后者爽快地答应。余凯回复没问题后，辛顿可能感受到了产业界的饥渴，就询问余凯是否介意自己问问其他家，比如Google。余凯后来在回忆道\[6\]：

“我当时有点儿后悔，猜我可能回答得太快了，让辛顿意识到了巨大的机会。但是，我也只能大度地说不介意。”

最终百度跟辛顿团队失之交臂。但对于这个结果，余凯并非没有心理准备。因为一方面辛顿有严重的腰椎间盘健康问题，不能开车，也不能坐飞机，很难承受跨越太平洋的中国之旅；另一方面，辛顿有太多的学生和朋友在Google工作了，双方渊源太深，其他三家本质上就是在陪标。

如果说AlexNet的影响还集中在学术圈的话，那么太浩湖的秘密拍卖则彻底震惊了产业界——因为Google在全球科技公司的眼皮子底下，花了4400万美元买了一家成立不到一个月、没有产品、没有收入，只有三个员工和几篇论文的公司。

最受刺激的显然是百度，虽然在拍卖上折戟，但百度管理层亲眼目睹了Google如何不惜代价投资深度学习，促使百度下定决心投入，并在2013年1月的年会上宣布成立深度学习研究院IDL。2014年5月，百度请来了“谷歌猫”项目的关键人物吴恩达，2017年1月，又请来了离开微软的陆奇。

而Google在拿下辛顿团队后再接再厉，在2014年以6亿美元买下了当年的竞拍对手DeepMind。

当时，马斯克向Google创始人Larry Page推荐了自己投资的DeepMind，为了能带上辛顿一起去伦敦验验成色，Google团队还专门包了架私人飞机，并且改造了座椅，解决辛顿不能坐飞机的问题\[6\]。

![](https://img.36krcdn.com/hsossms/20230907/v2_559119b170134c648ebf5f51ec07549a@000000_oswg508066oswg810oswg456_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

“英国选手”DeepMind在围棋比赛上战胜了李世石，2016年

和Google争夺DeepMind的是Facebook。当DeepMind花落Google后，扎克伯格转而挖来了“深度学习三巨头”之一的杨立昆。为了将杨立昆纳入麾下，扎克伯格答应了他许多苛刻要求，比如AI实验室设立在纽约，实验室与产品团队完全划清界限，允许杨立昆继续在纽约大学任职等等。

**2012年ImageNet挑战赛后，人工智能领域面临着非常严重的“人才供需错配”问题：**

由于推荐算法、图像识别、自动驾驶这些产业化空间被迅速打开，人才需求量暴增。但由于长期不被看好，深度学习的研究者是个很小的圈子，顶级学者更是两只手数得过来，供给严重不足。

这种情况下，如饥似渴的科技公司只能购买“人才期货”：**把教授挖过来，然后等他们把自己的学生也带进来。**

杨立昆加入Facebook后，先后有六名学生追随他入职。准备在造车上跃跃欲试的苹果挖来了辛顿的学生Ruslan Salakhutdinov，担任苹果首任AI总监。就连对冲基金Citadel也加入了抢人大战，挖走了当年和辛顿搞语音识别、后来还代表微软参与秘密竞拍的邓力。

![](https://img.36krcdn.com/hsossms/20230907/v2_bfb4730122f54d8b93c47bfd43ae4493@000000_oswg256527oswg1080oswg722_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

![](https://img.36krcdn.com/hsossms/20230907/v2_5ec6612f09a84522a7c577301f83aef0@000000_oswg163432oswg1080oswg499_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

![](https://img.36krcdn.com/hsossms/20230907/v2_82f0037e831346d89026fe2b499089fe@000000_oswg177816oswg1080oswg646_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

此后的历史我们再清楚不过：人脸识别、机器翻译、自动驾驶等产业化场景日行千里，GPU订单雪花一般飘向圣克拉拉的英伟达总部，人工智能的理论大厦也在日复一日的浇筑。

2017年，Google在论文《Attention is all you need》里提出Transformer模型，开启了如今的大模型时代。几年后，ChatGPT横空出世。

**而这一切的诞生，都可以追溯到2012年的ImageNet挑战赛。**

那么，推动2012年“大爆炸时刻”诞生的历史进程，又是在哪一年显现的呢？

答案是2006年。

## **伟大**

在2006年之前，深度学习的现状可以借用开尔文男爵的那句名言来概括：深度学习的大厦已经基本建成了，只不过在阳光灿烂的天空下，漂浮着三朵小乌云。

这三朵小乌云就是算法、算力和数据。

正如前文所说，由于模拟了人脑的机制，深度学习是一种在理论上非常完美的方案。但问题在于，无论是它需要吞噬的数据，还是需要消耗的算力，在当时都是一个科幻级别的规模，科幻到学术界对深度学习的主流看法是：**脑子正常的学者不会研究神经网络。**

但2006年发生的三件事改变了这一点：

**辛顿和学生Salakhutdinov（就是后来去苹果的那位）在Science上发表了论文Reducing the dimensionality of data with neural networks，第一次提出了有效解决梯度消失问题的解决方案，让算法层面迈出了一大步。**

![](https://img.36krcdn.com/hsossms/20230907/v2_3f0b6939dad244178dd303221bae01de@000000_oswg964087oswg1080oswg810_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)
Salakhutdinov（左一）与辛顿（中），2016年

**斯坦福大学的李飞飞意识到，如果数据规模难以还原现实世界的原貌，那么再好的算法也很难通过训练达到“模拟人脑”的效果。于是，她开始着手搭建ImageNet数据集。**

**英伟达发布Tesla架构的新款GPU，并随之推出CUDA平台，开发者利用GPU来训练深度神经网络的难度大幅度降低，望而生畏算力门槛被砍掉了一大截。**

这三件事的发生吹散了深度学习上空的三朵乌云，并在2012年的ImageNet挑战赛上交汇，彻底改写了高科技产业乃至整个人类社会的命运。

但在2006年，无论是杰夫·辛顿、李飞飞、黄仁勋，还是其他推动深度学习发展的人，显然都无法预料人工智能在此后的繁荣，更不用说他们所扮演的角色了。

![](https://img.36krcdn.com/hsossms/20230907/v2_a91bc26c62c34086ba8b0795bb6a7c34@000000_oswg487744oswg1080oswg652_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

Hinton和Salakhutdinov的论文

时至今日，AI为核心驱动的第四次工业革命又开启了，人工智能的演进速度只会越来越快。如果说我们能得到多少启发，也许不外乎以下三点：

**1.产业的厚度决定创新的高度。**

ChatGPT横空出世时，“为什么又是美国”的声音此起彼伏。但如果把时间拉长，会发现从晶体管、集成电路，到Unix、x86架构，再到如今的机器学习，美国学界和产业界几乎都是领跑者的角色。

这是因为，虽然关于美国“产业空心化”的讨论不绝于耳，但以软件为核心的计算机科学这门产业，不仅从未“外流”到其他经济体，反而优势越来越大。至今70多位ACM图灵奖的获得者，几乎全部是美国人。

吴恩达之所以选择Google合作“谷歌猫”项目，很大程度上是因为只有Google拥有算法训练所需要的数据和算力，而这又建立在Google强大的盈利能力的基础上。这就是产业厚度带来的优势——人才、投资、创新能力都会向产业的高地靠拢。

中国在自身的优势产业里，也在体现出这种“厚度优势”。当前最典型的就是新能源车，一边是欧洲车企包机来中国车展拜师新势力，一边是日本车企高管频繁跳槽到比亚迪——图什么呢？显然不是只图能在深圳交社保。

**2.越是前沿的技术领域，人才的重要性越大。**

Google之所以愿意花4400万美元买下辛顿的公司，是因为在深度学习这样的前沿技术领域，一个顶级学者的作用，往往大过一万个计算机视觉专业的应届生。假如当时竞拍成功的是百度或微软，人工智能的发展脉络可能都会被改写。

这种“为了你买下整个公司”的行为，其实非常常见。苹果自研芯片的关键阶段，顺手买了一家PASemi的小公司，就是为了把芯片架构大神Jim Keller挖到手——苹果的A4、AMD的Zen、特斯拉的FSD芯片，都得到了Jim Keller的技术扶贫。

这也是产业竞争力带来的最大优势——对人才的吸引力。

“深度学习三巨头”没有一个是美国人，AlexNet这个名字来自辛顿的学生Alex Krizhevsky，他出生在苏联治下的乌克兰，在以色列长大，来加拿大读书。更不用说如今还活跃在美国高科技公司的众多华人面孔。

**3.创新的难度在于，如何面对不确定性。**

除了“人工智能之父”Marvin Minsky反对深度学习之外，另一个知名深度学习反对者是加州大学伯克利分校的Jitendra Malik，辛顿和吴恩达都被他冷嘲热讽过。李飞飞在搭建ImageNet时也曾咨询过Malik，后者给她的建议是：Do something more useful（做点更有用的事）。

![](https://img.36krcdn.com/hsossms/20230907/v2_7824d396b06443248ef652175db3ebe1@000000_oswg306394oswg1080oswg619_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

李飞飞Ted演讲

正是这些行业先驱的不看好，导致深度学习经历了数十年的万马齐喑。即便到了2006年辛顿撕开了一束曙光，三巨头的另一位杨立昆还在反复向学术界证明“深度学习也有研究价值”。

杨立昆从80年代就开始研究神经网络，在贝尔实验室期间，杨立昆就和同事设计了一种名叫ANNA的芯片，试图解决算力问题。后来AT&T由于经营压力要求研究部门“赋能业务”，杨立昆的回答是“我就是要研究计算机视觉，有本事你解雇我”。最终求锤得锤，喜提N+1\[6\]。

任何前沿技术领域的研究者都必须面对一个问题——**如果这个东西做不出来怎么办？**

从1972年进入爱丁堡大学算起，辛顿在深度学习的前线已经鏖战了50年。2012年ImageNet挑战赛举办时，他已经65岁了。很难想象他在漫长的时间里面对学术界的种种质疑，需要消解多少自我怀疑与否定。

如今我们知道，2006年的辛顿已经坚持到了黎明前最后的黑暗，但他自己也许并不知道这一点，更不用说整个学术界和产业界。就像2007年iPhone发布时，大多数人的反应可能和时任微软CEO鲍尔默是一样的：

![](https://img.36krcdn.com/hsossms/20230907/v2_dd863c61057842459937f1de1239d94d@000000_oswg1016138oswg1080oswg1286_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1)

目前，iPhone依然是世界上最贵的手机，而且没有键盘

推动历史的人，往往猜不到自己在历史进程中的坐标。

伟大之所以为伟大，不是因为其横空出世时的惊艳，而是因为它要在无边黑暗中，忍受漫长的籍籍无名与不被理解。直到多年之后，人们才能顺着这些标尺，感叹那时群星璀璨，天才辈出。

一个又一个科学研究的领域里，无数的学者终其一生都不曾窥见希望的微光。因而从某种角度看，辛顿和其他深度学习推动者是幸运的，他们创造了伟大，间接推动了产业界一个又一个成功。

资本市场会给成功定一个公允的价格，历史则记录那些创造伟大的孤独和汗水。

**参考资料**

\[1\] 16000台电脑一起找猫，纽约时报

\[2\] Fei-Fei Li's Quest to Make AI Better for Humanity，Wired

\[3\] 李飞飞的TED演讲

\[4\] 21秒看尽ImageNet屠榜模型，60+模型架构同台献艺，机器之心

\[5\] 卷积神经网络的“封神之路”：一切始于AlexNet，新智元

\[6\] 深度学习革命，凯德·梅茨

\[7\] To Find AI Engineers, Google and Facebook Hire Their Professors，The Information

\[8\] 深度学习三十年创新路，朱珑

\[9\] ImageNet这八年：李飞飞和她改变的AI世界，量子位

\[10\] DEEP LEARNING: PREVIOUS AND PRESENT APPLICATIONS，Ramiro Vargas

\[11\] Review of deep learning: concepts, CNN architectures, challenges, applications, future directions，Laith Alzubaidi等

\[12\] Literature Review of Deep Learning Research Areas，Mutlu Yapıcı等

\[13\] ChatGPT背后真正的英雄：OpenAI首席科学家Ilya Sutskever的信仰之跃，新智元

\[14\] 10 years later, deep learning ‘revolution’ rages on, say AI pioneers Hinton, LeCun and Li，Venturebeat

\[15\] From not working to neural networking，经济学人

\[16\] Huge “foundation models” are turbo-charging AI progress，经济学人

\[17\] 2012: A Breakthrough Year for Deep Learning，Bryan House

\[18\] 深度学习：人工智能的“神奇魔杖”，安信证券

\[19\] 深度学习算法发展:从多样到统一 ，国金证券

本文来自微信公众号[“远川研究所”（ID：caijingyanjiu）](http://mp.weixin.qq.com/s?__biz=MzIwMDY2NTgwMA==&mid=2247517796&idx=1&sn=78c9db7615cfb8b18d0aad7340451743&chksm=96fb79c3a18cf0d5558dc36c21f9e2a77f583322851954226d6dec26a0958e0f48b863ef92b9&scene=0&xtrack=1#rd)，作者：李墨天，编辑：戴老板，视觉设计：疏睿，研究支持：陈彬、陈畅，36氪经授权发布。
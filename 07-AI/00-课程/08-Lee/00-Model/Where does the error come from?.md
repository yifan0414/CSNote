---
created: 2025-04-04 10:05
tags:
---
所谓机器学习，就是建立一个 `model`，拟合已有数据，预测没有见过的数据。其中评价一个 `model` 好坏的标准称为：`loss function`。

`ERROR` 主要来自于两个地方：`bias` 和 `variance`。由概率论可以知道，`bias` 主要是样本均值与 `real result` 的偏差，` variance ` 是样本方差。我们的目的是用**样本**来估计**总体**，所以需要尽可能降低 `bias` 和 `variance`。但现实世界中的 `model` 往往是要在这两种标准之间做 **trade-off**。

![[Bias and Variance (v2).pdf#page=7&rect=58,12,683,525|Bias and Variance (v2), p.7]]

## 欠拟合和过拟合
![[Bias and Variance (v2).pdf#page=15&rect=26,14,704,491|Bias and Variance (v2), p.15]]


> [!NOTE] chatgpt 总结
> 在机器学习中，**模型（Model）** 是通过训练数据学习并在新数据上进行预测的系统。通过训练，我们希望模型能够从现有的样本数据中找出数据的规律，以便在遇到未见过的新数据时进行准确的预测。评价一个模型的好坏的标准通常是**损失函数（Loss Function）**，它量化了模型预测与真实值之间的差距。
> 
> ### 1. **模型误差的来源**
> 
> 模型的误差可以分为两部分：
> - **偏差（Bias）**：偏差指的是模型的预测值与真实值之间的差距。简单来说，它衡量了模型是否能够正确地捕捉到数据的规律。如果偏差很大，说明模型无法准确地拟合训练数据。
>   
>   - **高偏差的表现**：通常发生在模型过于简单，无法捕捉数据的复杂模式时。这种情况通常是**欠拟合**，模型未能有效学习数据中的重要特征。
>   
>   - **低偏差的表现**：模型能够较好地拟合数据，接近真实结果，但这并不意味着它就能泛化到新的数据上。
> 
> - **方差（Variance）**：方差指的是模型在不同的训练集上对结果的敏感度，衡量的是模型在不同数据集上的表现波动。如果方差很大，说明模型在某个特定的训练集上可能表现很好，但在新的数据集上可能无法表现得很好。这种情况通常是**过拟合**，模型记住了训练数据中的噪声，而不是捕捉到数据的真实规律。
> 
>   - **高方差的表现**：通常发生在模型过于复杂，能够在训练集上完美拟合，但对于新数据的预测能力差。
>   
>   - **低方差的表现**：模型的表现比较稳定，即使数据有所变动，模型的预测结果也不会发生剧烈变化。
> 
> ### 2. **偏差-方差权衡（Bias-Variance Tradeoff）**
> 
> 在机器学习中，**偏差和方差之间存在权衡关系**。我们希望找到一个模型，使得它既有较低的偏差，也有较低的方差，但现实中往往难以做到这一点。为了在这两者之间做出平衡，我们需要理解以下几个方面：
> 
> - **简单模型（高偏差，低方差）**：如果使用简单的模型（比如线性回归、低阶多项式等），它们可能无法充分捕捉数据的复杂性，导致偏差较高。但是，由于模型简单，它在不同训练集上的表现较为一致，因此方差较低。通常这种情况是欠拟合。
> 
> - **复杂模型（低偏差，高方差）**：复杂的模型（比如深度神经网络、非常深的决策树等）有很强的拟合能力，能够准确地拟合训练数据，但它们容易过度拟合训练集中的噪声，导致方差较高。通常这种情况是过拟合。
> 
> ### 3. **理想的模型：低偏差和低方差**
> 
> 理想的机器学习模型应该在低偏差和低方差之间找到一个平衡点，能够较好地拟合训练数据，并且对未见过的数据（测试数据）具有良好的泛化能力。
> 
> - **泛化能力**：一个好的模型不仅仅在训练数据上表现良好，还需要在新数据（如测试集）上有很好的表现，这就是泛化能力。
>   
> - **过拟合与欠拟合的平衡**：通过调整模型的复杂度、正则化手段和训练过程，我们可以控制模型的偏差和方差，避免过拟合和欠拟合。例如，使用正则化技术（如 L2 正则化、Dropout 等）可以帮助控制模型的复杂度，从而减少方差；而使用简化的模型（例如降低模型的多项式阶数）可以帮助减少偏差。
> 
> ### 4. **降低偏差和方差的方法**
> 
> - **降低偏差（减少欠拟合）**：
>   - 使用更复杂的模型（例如深度神经网络、非线性模型等）。
>   - 增加更多的特征，以便捕捉更多的信息。
>   - 进行更深入的特征工程。
> 
> - **降低方差（减少过拟合）**：
>   - 使用正则化方法（如 L1、L2 正则化、Dropout 等）来限制模型复杂度。
>   - 使用更多的数据进行训练，数据越多，模型越能捕捉到数据的真实规律，而非噪声。
>   - 使用交叉验证来选择模型超参数，以便找到最优的模型复杂度。
>   - 采用集成方法（如随机森林、XGBoost、集成学习）来减少方差。
> 
> ### 5. **偏差-方差分解**
> 
> 实际中，我们常常使用**偏差-方差分解（Bias-Variance Decomposition）**来分析模型误差。假设我们的目标是最小化**总体预测误差**（总损失）：
> 
> $$
> \text{Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
> $$
> 
> - **Irreducible Error**：是数据本身的噪声，无法通过任何模型降低。
> - **Bias²**：是由于模型本身的假设过于简单，导致预测结果偏离真实值的程度。
> - **Variance**：是由于模型在训练数据上的过度拟合，导致不同训练集间预测结果的波动性。
> 
> ### 总结
> 
> 在机器学习中，误差来源于偏差和方差。偏差通常是由于模型的假设过于简单而导致的欠拟合，而方差则通常与模型的复杂度过高和过拟合相关。我们希望找到一个理想的平衡点，既能够降低偏差，又能够减少方差。这通常需要通过模型选择、正则化、特征选择、数据集大小等多方面的技术来实现。
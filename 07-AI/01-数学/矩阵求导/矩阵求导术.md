---
创建时间: 2025-07-22 11:41
tags: 
link: https://zhuanlan.zhihu.com/p/24709748
---
矩阵求导 ${ }^{+}$ 的技术，在统计学、控制论、机器学习等领域有广泛的应用。鉴于我看过的一些资料或言之不详、或繁乱无绪，本文来做个科普，分作两篇，上篇讲标量对矩阵的求导术，下篇讲矩阵对矩阵的求导术。本文使用小写字母 $\times$ 表示标量，粗体小写字母 $\boldsymbol{x}$ 表示（列）向量，大写字母$X$表示矩阵。
首先来琢磨一下定义，标量 $f$ 对矩阵 $X$ 的导数，定义为 $\dfrac{\partial f}{\partial X}=\left[\dfrac{\partial f}{\partial X_{i j}}\right]$ ，即 $f$ 对 $X$ 逐元素求导排成与 $X$ 尺寸相同的矩阵。然而，这个定义在计算中并不好用，实用上的原因是对函数较复杂的情形难以逐元素求导；哲理上的原因是逐元素求导破坏了整体性。试想，为何要将f看做矩阵 $X$ 而不是各元素 $X_{i j}$ 的函数呢？**答案是用矩阵运算更整洁。所以在求导时不宜拆开矩阵，而是要找一个从整体出发的算法。**

为此，我们来回顾
一元微积分中的导数（标量对标量的导数）与微分有联系：
$$
\begin{align*}d f=f^{\prime}(x) d x\end{align*}
$$
多元微积分中的梯度 ${ }^{+}$（标量对向量的导数）也与微分有联系：
$$
d f=\sum_{i=1}^n \frac{\partial f}{\partial x_i} d x_i=\frac{\partial f}{\partial \boldsymbol{x}}^T d \boldsymbol{x}
$$
这里第一个等号是全微分公式 ${ }^{+}$，第二个等号表达了梯度与微分的联系：全微分 $d f$ 是梯度向量 $\frac{\partial f}{\partial x}(n \times 1)$ 与微分向量 $d x(n \times 1)$ 的内积；

受此启发，我们将矩阵导数与微分建立联系：
$$
d f=\sum_{i=1}^m \sum_{j=1}^n \frac{\partial f}{\partial X_{i j}} d X_{i j}=\operatorname{tr}\left(\left(\dfrac{\partial f}{\partial X}\right)^T d X\right)
$$ 
其中 $\mathrm{tr}$ 代表迹 ${ }^{+}$（trace）是方阵对角线元素之和，满足性质：**对尺寸相同的矩阵 $A, B$ ， $\operatorname{tr}\left(A^T B\right)=\sum_{i, j} A_{i j} B_{i j}$ ，即 $\operatorname{tr}\left(A^T B\right)$ 是矩阵 $A,B$ 的内积。** 与梯度相似，这里第一个等号是全微分公式，第二个等号表达了矩阵导数与微分的联系：全微分 $d f$ 是导数 $\dfrac{\partial f}{\partial X}(m \times n)$ 与微分矩阵 $d X(m \times n)$ 的内积。

然后来**建立运算法则**。回想遇到较复杂的一元函数如 $f=\log (2+\sin x) e^{\sqrt{x}}$ ，我们是如何求导的呢？通常不是从定义开始求极限，而是先建立了初等函数求导和四则运算、复合等法则，再来运用这些法则。故而，我们来创立常用的矩阵微分的运算法则：

1. **加减法**：$d(X \pm Y)=d X \pm d Y$ ；矩阵乘法：$d(X Y)=(d X) Y+X d Y$ ；转置： $d\left(X^T\right)=(d X)^T$ ；迹：$d \operatorname{tr}(X)=\operatorname{tr}(d X)$ 。

2. **逆**：$d X^{-1}=-X^{-1} d X X^{-1}$ 。此式可在 $X X^{-1}=I$ 两侧求微分来证明。

3. **行列式**：$d|X|=\operatorname{tr}\left(X^{\#} d X\right)$ ，其中 $X^{\#}$ 表示 X 的伴随矩阵 ${ }^{+}$，在 $X$ 可逆时又可以写作 $d|X|=|X| \operatorname{tr}\left(X^{-1} d X\right)$ 。^[此式可用 Laplace 展开来证明，详见张贤达《矩阵分析与应用》第 279 页。]

4. **逐元素乘法**：$d(X \odot Y)=d X \odot Y+X \odot d Y, ~ \odot$ 表示尺寸相同的矩阵 $X, Y$ 逐元素相乘。

5. **逐元素函数**：$d \sigma(X)=\sigma^{\prime}(X) \odot d X, \sigma(X)=\left[\sigma\left(X_{i j}\right)\right]$ 是逐元素标量函数运算， $\sigma^{\prime}(X)=\left[\sigma^{\prime}\left(X_{i j}\right)\right]$ 是逐元素求导数。例如

$$
\begin{align*}X&=\left[\begin{array}{ll}
X_{11} & X_{12} \\
X_{21} & X_{22}
\end{array}\right]\\ d \sin (X)&=\left[\begin{array}{ll}
\cos X_{11} d X_{11} & \cos X_{12} d X_{12} \\
\cos X_{21} d X_{21} & \cos X_{22} d X_{22}
\end{array}\right]\\&=\cos (X)\odot d X\end{align*}
$$
我们试图利用矩阵导数与微分的联系 $d f=\operatorname{tr}\left(\left(\dfrac{\partial f}{\partial X}\right)^T d X\right)$ ，在求出左侧的微分 $d f$ 后，该如何写成右侧的形式并得到导数呢？这需要一些迹技巧（trace trick）：
1. **标量套上迹**：$a=\operatorname{tr}(a)$
2. **转置**： $\operatorname{tr}\left(A^T\right)=\operatorname{tr}(A)$ 。
3. **线性**： $\operatorname{tr}(A \pm B)=\operatorname{tr}(A) \pm \operatorname{tr}(B)$ 。
4. **矩阵乘法交换**： $\operatorname{tr}(A B)=\operatorname{tr}(B A)$ ，其中 $A$ 与 $B^T$ 尺寸相同。两侧都等于 $\sum_{i, j} A_{i j} B_{j i}$ 。
5. ***矩阵乘法／逐元素乘法交换***： $\operatorname{tr}\left(A^T(B \odot C)\right)=\operatorname{tr}\left((A \odot B)^T C\right)$ ，其中 $A, B, C$ 尺寸相同。两侧都等于 $\sum_{i, j} A_{i j} B_{i j} C_{i j}$ 。

观察一下可以断言，**若标量函数 $f$ 是矩阵 $X$ 经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对 $f$ 求微分，再使用迹技巧给 $df$ 套上迹并将其它项交换至 $dX$ 左侧，对照导数与微分的联系 $d f=\operatorname{tr}\left(\left(\dfrac{\partial f}{\partial X}\right)^T d X\right)$ ，即能得到导数。**

特别地，若矩阵退化为向量，对照导数与微分的联系 $d f=\dfrac{\partial f^T}{\partial x} d x$ ，即能得到导数。

在建立法则的最后，来谈一谈***复合***：**假设已求得 $\dfrac{\partial f}{\partial Y}$ ，而 $Y$ 是 $X$ 的函数，如何求 $\frac{\partial f}{\partial X}$ 呢？** 在微积分中有标量求导的链式法则 $\dfrac{\partial f}{\partial x}=\dfrac{\partial f}{\partial y} \dfrac{\partial y}{\partial x}$ ，但这里我们不能随意沿用标量的链式法则，因为矩阵对矩阵的导数 $\dfrac{\partial Y}{\partial X}$ 截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出 $d f=\operatorname{tr}\left(\dfrac{\partial f}{\partial Y}^T d Y\right)$ ，再将 $d Y$ 用 $dX$ 表示出来代入，并使用迹技巧将其他项交换至 $dX$ 左侧，即可得到 $\dfrac{\partial f}{\partial X}$ 。

最常见的情形是 $Y=A X B$ ，此时

$$
\begin{align*}
& d f=\operatorname{tr}\left(\frac{\partial f}{\partial Y}^T d Y\right)=\operatorname{tr}\left(\frac{\partial f}{\partial Y}^T A d X B\right)=\operatorname{tr}\left(B \frac{\partial f}{\partial Y}^T A d X\right) \\
& =\operatorname{tr}\left(\left(A^T \frac{\partial f}{\partial Y} B^T\right)^T d X\right)
\end{align*}
$$

可得到 $\dfrac{\partial f}{\partial X}=A^T \dfrac{\partial f}{\partial Y} B^T$ 。注意这里 $d Y=(d A) X B+A d X B+A X d B=A d X B$ ，由于 $A, B$ 是常量，$d A=0, d B=0$ ，以及我们使用矩阵乘法交换的迹技巧交换了 $\dfrac{\partial f}{\partial Y}^T A d X$ 与 $B$ 。

### 例子

#### 例 1
$$f=\boldsymbol{a}^T X \boldsymbol{b} ,\text{求} \dfrac{\partial f}{\partial X}$$
其中 $\boldsymbol{a}$ 是 $m \times 1$ 列向量，$X$ 是 $m \times n$ 矩阵， $\boldsymbol{b}$ 是 $n \times 1$ 列向量， $f$ 是标量。

**解**：

1. 先使用矩阵乘法法则求微分，$d f=d \boldsymbol{a}^T X \boldsymbol{b}+\boldsymbol{a}^T d X \boldsymbol{b}+\boldsymbol{a}^T X d \boldsymbol{b}=\boldsymbol{a}^T d X \boldsymbol{b}$ ，注意这里的 $\boldsymbol{a}, \boldsymbol{b}$ 是常量，$d \boldsymbol{a}=\mathbf{0}, d \boldsymbol{b}=\mathbf{0}$ 。由于 $df$ 是标量，它的迹等于自身，$d f=\operatorname{tr}(d f)$

2. 套上迹并做矩阵乘法交换：$d f=\operatorname{tr}\left(\boldsymbol{a}^T d X \boldsymbol{b}\right)=\operatorname{tr}\left(\boldsymbol{b} \boldsymbol{a}^T d X\right)=\operatorname{tr}\left(\left(\boldsymbol{a} \boldsymbol{b}^T\right)^T d X\right)$ ，注意这里我们根据 $\operatorname{tr}(A B)=\operatorname{tr}(B A)$ 交换了 $\boldsymbol{a}^T d X$ 与 $\boldsymbol{b}$ 

3. 对照导数与微分的联系 $d f=\operatorname{tr}\left(\dfrac{\partial f^T}{\partial X} d X\right)$ ，得到 $\dfrac{\partial f}{\partial X}=\boldsymbol{a} \boldsymbol{b}^T$ 。

> 注意：这里不能用 $\dfrac{\partial f}{\partial X}=\boldsymbol{a}^T \dfrac{\partial X}{\partial X} \boldsymbol{b}=$ ？，导数与矩阵乘法的交换是不合法则的运算（而微分是合法的）。有些资料在计算矩阵导数时，会略过求微分这一步，这是逻辑上解释不通的。

#### 例 2
$$
f=\boldsymbol{a}^T \exp (X \boldsymbol{b}),\text{求} \dfrac{\partial f}{\partial X}
$$
其中 $\boldsymbol{a}$ 是 $m \times 1$ 列向量，$X$ 是 $m \times n$ 矩阵， $\boldsymbol{b}$ 是 $n \times 1$ 列向量， $\exp$ 表示逐元素求指数，$f$ 是标量。

**解**：

1. 先使用矩阵乘法、逐元素函数法则求微分：$d f=\boldsymbol{a}^T(\exp (X \boldsymbol{b}) \odot(d X \boldsymbol{b}))$ 

2. 再套上迹并做交换：$d f=\operatorname{tr}\left(\boldsymbol{a}^T(\exp (X \boldsymbol{b}) \odot(d X \boldsymbol{b}))\right)=\operatorname{tr}\left((\boldsymbol{a} \odot \exp (X \boldsymbol{b}))^T d X \boldsymbol{b}\right)$ $=\operatorname{tr}\left(\boldsymbol{b}(\boldsymbol{a} \odot \exp (X \boldsymbol{b}))^T d X\right)=\operatorname{tr}\left(\left((\boldsymbol{a} \odot \exp (X \boldsymbol{b})) \boldsymbol{b}^T\right)^T d X\right)$ ，注意这里我们先根据 $\operatorname{tr}\left(A^T(B \odot C)\right)=\operatorname{tr}\left((A \odot B)^T C\right)$ 交换了 $\boldsymbol{a}, \exp (X \boldsymbol{b})$ 与 $d X \boldsymbol{b}$ ，

3. 再根据 $\operatorname{tr}(A B)=\operatorname{tr}(B A)$ 交换了 $(\boldsymbol{a} \odot \exp (X \boldsymbol{b}))^T d X$ 与 $\boldsymbol{b}$ 。

4. 对照导数与微分的联系 $d f=\operatorname{tr}\left(\left(\dfrac{\partial f}{\partial X}\right)^T d X\right)$ ，得到 $\dfrac{\partial f}{\partial X}=(\boldsymbol{a} \odot \exp (X \boldsymbol{b})) \boldsymbol{b}^T$ 。

#### 例 3

$$
f=\operatorname{tr}\left(Y^T M Y\right), Y=\sigma(W X) ,\text{求} \frac{\partial f}{\partial X}
$$ 
其中 $W$ 是 $l \times m$ 矩阵，$X$ 是 $m \times n$ 矩阵，$Y$ 是 $l \times n$ 矩阵，$M$ 是 $l \times l$ 对称矩阵，$\sigma$ 是逐元素函数，$f$ 是标量。

**解**：

1. 先求 $\dfrac{\partial f}{\partial Y}$ ，求微分，使用矩阵乘法、转置法则：
$$
\begin{align*}
& d f=\operatorname{tr}\left((d Y)^T M Y\right)+\operatorname{tr}\left(Y^T M d Y\right)=\operatorname{tr}\left(Y^T M^T d Y\right)+\operatorname{tr}\left(Y^T M d Y\right) \\
& =\operatorname{tr}\left(Y^T\left(M+M^T\right) d Y\right)
\end{align*}
$$

2. 对照导数与微分的联系，得到 $\dfrac{\partial f}{\partial Y}=\left(M+M^T\right) Y=2 M Y$ ，注意这里 $M$ 是对称矩阵。

3. 为求 $\dfrac{\partial f}{\partial X}$ ，写出 $d f=\operatorname{tr}\left(\dfrac{\partial f}{\partial Y}^T d Y\right)$ ，再将 $dY$ 用 $dX$ 表示出来代入，并使用矩阵乘法／逐元素乘法交换：
$$
d f=\operatorname{tr}\left(\frac{\partial f}{\partial Y}^T\left(\sigma^{\prime}(W X) \odot(W d X)\right)\right)=\operatorname{tr}\left(\left(\frac{\partial f}{\partial Y} \odot \sigma^{\prime}(W X)\right)^T W d X\right)
$$

4. 对照导数与微分的联系，得到
$$
\frac{\partial f}{\partial X}=W^T\left(\frac{\partial f}{\partial Y} \odot \sigma^{\prime}(W X)\right)=W^T\left((2 M \sigma(W X)) \odot \sigma^{\prime}(W X)\right)
$$

#### 例 4 线性回归

$$
l=\|X \boldsymbol{w}-\boldsymbol{y}\|^2, \text{求} \boldsymbol{w} \text{的最小二乘估计,即求} \frac{\partial l}{\partial \boldsymbol{w}} \text{的零点}
$$
 其中 $\boldsymbol{y}$ 是 $m \times 1$ 列向量，$X$ 是 $m \times n$ 矩阵， $\boldsymbol{w}$ 是 $n \times 1$ 列向量，$l$ 是标量。

**解**：这是标量对向量的导数，不过可以把向量看做矩阵的特例。

1. 先将向量模平方改写成向量与自身的内积：$l=(X \boldsymbol{w}-\boldsymbol{y})^T(X \boldsymbol{w}-\boldsymbol{y})$ ，求微分，使用矩阵乘法、转置等法则： $$d l=(X d \boldsymbol{w})^T(X \boldsymbol{w}-\boldsymbol{y})+(X \boldsymbol{w}-\boldsymbol{y})^T(X d \boldsymbol{w})=2(X \boldsymbol{w}-\boldsymbol{y})^T X d \boldsymbol{w}$$ 注意这里 $X d \boldsymbol{w}$ 和 $X \boldsymbol{w}-\boldsymbol{y}$ 是向量，两个向量的内积满足 $\boldsymbol{u}^T \boldsymbol{v}=\boldsymbol{v}^T \boldsymbol{u}$ 。

2. 对照导数与微分的联系 $d l=\left(\dfrac{\partial l}{\partial \boldsymbol{w}}{ }\right)^T d \boldsymbol{w}$ ，得到 $\dfrac{\partial l}{\partial \boldsymbol{w}}=2 X^T(X \boldsymbol{w}-\boldsymbol{y})$ 。 $\dfrac{\partial l}{\partial \boldsymbol{w}}=\mathbf{0}$ 即 $X^T X \boldsymbol{w}=X^T \boldsymbol{y}$ ，得到 $\boldsymbol{w}$ 的最小二乘估计为 $\boldsymbol{w}=\left(X^T X\right)^{-1} X^T \boldsymbol{y}$ 。

#### 例5  方差的最大似然估计 ${ }^{+}$ 

样本 $\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)$ ，求方差 $\Sigma$ 的最大似然估计。也就是
$$
l=\log |\Sigma|+\frac{1}{N} \sum_{i=1}^N\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)^T \Sigma^{-1}\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right) ,\text{求} \frac{\partial l}{\partial \Sigma} \text{的零点}
$$

其中 $\boldsymbol{x}_i$ 是 $m \times 1$ 列向量，$\overline{\boldsymbol{x}}=\frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i$ 是样本均值，$\sum$ 是 $m \times m$ 对称正定矩阵，$l$ 是标量， $\log$ 表示自然对数。

**解**：

1. 首先求微分，使用矩阵乘法、行列式、逆等运算法则，第一项是 
$$
d \log |\Sigma|=|\Sigma|^{-1} d|\Sigma|=\operatorname{tr}\left(\Sigma^{-1} d \Sigma\right)
$$ 
第二项是
$$
\begin{align*}
\frac{1}{N} \sum_{i=1}^N\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)^T d \Sigma^{-1}\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)=-\frac{1}{N} \sum_{i=1}^N\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)^T \Sigma^{-1} d \Sigma \Sigma^{-1}\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)
\end{align*}
$$

2. 再给第二项套上迹做交换： 
$$
\begin{align*}
\operatorname{tr}&\left(\frac{1}{N} \sum_{i=1}^N\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)^T \Sigma^{-1} d \Sigma \Sigma^{-1}\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)\right) \\&=\frac{1}{N} \sum_{i=1}^N \operatorname{tr}\left(\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)^T \Sigma^{-1} d \Sigma \Sigma^{-1}\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)\right) \\ &=\frac{1}{N} \sum_{i=1}^N \operatorname{tr}\left(\Sigma^{-1}\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)^T \Sigma^{-1} d \Sigma\right)\\ &=\operatorname{tr}\left(\Sigma^{-1} S \Sigma^{-1} d \Sigma\right)
\end{align*}
$$

3. 其中先交换迹与求和，然后将 $\Sigma^{-1}\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)$ 交换到左边，最后再交换迹与求和，并定义样本方差矩阵为
$$
		S=\frac{1}{N} \sum_{i=1}^N\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)\left(\boldsymbol{x}_i-\overline{\boldsymbol{x}}\right)^T
$$

4. 得到 $d l=\operatorname{tr}\left(\left(\Sigma^{-1}-\Sigma^{-1} S \Sigma^{-1}\right) d \Sigma\right)$ 。

5. 对照导数与微分的联系，有 $\dfrac{\partial l}{\partial \Sigma}=\left(\Sigma^{-1}-\Sigma^{-1} S \Sigma^{-1}\right)^T$ ，其零点即 $\Sigma$ 的最大似然估计为 $\Sigma=S$ 。

#### 例 6 多元 logistic 回归

$$
l=-\boldsymbol{y}^T \log \operatorname{softmax}(W \boldsymbol{x}), \text{求} \frac{\partial l}{\partial W}
$$ 
其中 $\boldsymbol{y}$ 是除一个元素为 $1$ 外其它元素为 $0$ 的 $m \times 1$ 列向量，$W$ 是 $m \times n$ 矩阵， $\boldsymbol{x}$ 是 $n \times 1$ 列向量，$l$ 是标量； $\log$ 表示自然对数， $\operatorname{softmax}(\boldsymbol{a})=\dfrac{\exp (\boldsymbol{a})}{\mathbf{1}^T \exp (\boldsymbol{a})}$ ，其中 $\exp (\boldsymbol{a})$ 表示逐元素求指数， $1$ 代表全 $1$ 向量。
> [!note]- softmax 解释 
> 
>  **✅ 1. 记号解释：**
> 
> * $\boldsymbol{a}$ 是一个 **列向量**（比如 $n \times 1$ 维）。
> * $\exp(\boldsymbol{a})$：表示对 $\boldsymbol{a}$ 中的每个元素进行指数运算，也就是：
> 
>   $$
>   \exp(\boldsymbol{a}) = \begin{bmatrix} \exp(a_1) \\ \exp(a_2) \\ \vdots \\ \exp(a_n) \end{bmatrix}
> $$
> * $\mathbf{1}^T$：是一个 **行向量**，里面所有元素都是 1，也就是：
> 
>  $$
>   \mathbf{1}^T = \begin{bmatrix} 1 & 1 & \cdots & 1 \end{bmatrix}
> $$
> 
>  **✅ 2. 矩阵乘法含义：**
> 
> 当我们写：
> 
> $$
> \mathbf{1}^T \exp(\boldsymbol{a})
> $$
> 
> 其实是在进行矩阵乘法：一个 $1 \times n$ 的行向量（全是1）左乘一个 $n \times 1$ 的列向量（每个元素是 $\exp(a_i)$），结果是一个 **标量**，也就是：
> 
> $$
> \mathbf{1}^T \exp(\boldsymbol{a}) = \sum_{i=1}^n \exp(a_i)
> $$
> 等价于
> $$
> \operatorname{softmax}(\boldsymbol{a})=\frac{\exp (\boldsymbol{a})}{\sum_i \exp \left(a_i\right)}
> $$

**解 1**：

1. 首先将 $\operatorname{softmax}$ 函数代入并写成
$$
\begin{align*}
l&=-\boldsymbol{y}^T\left(\log (\exp (W \boldsymbol{x}))-\mathbf{1} \log \left(\mathbf{1}^T \exp (W \boldsymbol{x})\right)\right)\\&=-\boldsymbol{y}^T W \boldsymbol{x}+\log \left(\mathbf{1}^T \exp (W \boldsymbol{x})\right)
\end{align*}
$$
这里要注意逐元素 $\log$ 满足等式 $\log (\boldsymbol{u} / c)=\log (\boldsymbol{u})-1 \log (c)$ ，以及 $\boldsymbol{y}$ 满足 $\boldsymbol{y}^T \mathbf{1}=1$ （ $\boldsymbol{y}$ 是除一个元素为 $1$ 外其它元素为 $0$ 的 $m \times 1$ 列向量）

2. 求微分，使用矩阵乘法、逐元素函数等法则
$$
d l=-\boldsymbol{y}^T d W \boldsymbol{x}+\frac{\mathbf{1}^T(\exp (W \boldsymbol{x}) \odot(d W \boldsymbol{x}))}{\mathbf{1}^T \exp (W \boldsymbol{x})}
$$

3. 再套上迹并做交换，注意可化简 $\mathbf{1}^T(\exp (W \boldsymbol{x}) \odot(d W \boldsymbol{x}))=\exp (W \boldsymbol{x})^T d W \boldsymbol{x}$ ，这是根据等式 $1^T(\boldsymbol{u} \odot \boldsymbol{v})=\boldsymbol{u}^T \boldsymbol{v}$

4. 故
$$
\begin{align*}
d l&=\operatorname{tr}\left(-\boldsymbol{y}^T d W \boldsymbol{x}+\frac{\exp (W \boldsymbol{x})^T d W \boldsymbol{x}}{\mathbf{1}^T \exp (W \boldsymbol{x})}\right)\\&=\operatorname{tr}\left(-\boldsymbol{y}^T d W \boldsymbol{x}+\operatorname{softmax}(W \boldsymbol{x})^T d W \boldsymbol{x}\right) \\
& =\operatorname{tr}\left(\boldsymbol{x}(\operatorname{softmax}(W \boldsymbol{x})-\boldsymbol{y})^T d W\right)
\end{align*}
$$


5. 对照导数与微分的联系，得到 
 $$
 \color{blue}{\dfrac{\partial l}{\partial W}=(\operatorname{softmax}(W \boldsymbol{x})-\boldsymbol{y}) \boldsymbol{x}^T}
 $$

**解 2**：
定义 $\boldsymbol{a}=W \boldsymbol{x}$ ，则 $l=-\boldsymbol{y}^T \log \operatorname{softmax}(\boldsymbol{a})$ ，先同上求出 $\dfrac{\partial l}{\partial \boldsymbol{a}}=\operatorname{softmax}(\boldsymbol{a})-\boldsymbol{y}$ ，再利用复合法则：$d l=\operatorname{tr}\left(\left(\dfrac{\partial l}{\partial \boldsymbol{a}}\right)^T d\boldsymbol{a}\right)=\operatorname{tr}\left(\left(\dfrac{\partial l}{\partial \boldsymbol{a}}\right)^T d W \boldsymbol{x}\right)=\operatorname{tr}\left(\boldsymbol{x} \left(\dfrac{\partial l}{\partial \boldsymbol{a}}\right)^T d W\right)$ ，得到 $\dfrac{\partial l}{\partial W}=\dfrac{\partial l}{\partial \boldsymbol{a}} \boldsymbol{x}^T{ }_。$

#### 例 7 二层神经网络

> [!quote] [[07-AI/01-数学/矩阵求导/反向传播|反向传播]]

$$l=-\boldsymbol{y}^T \log \operatorname{softmax}\left(W_2 \sigma\left(W_1 \boldsymbol{x}\right)\right), \text{求} \frac{\partial l}{\partial W_1} \text{和} \frac{\partial l}{\partial W_2}$$
其中 $\boldsymbol{y}$ 是除一个元素为 $1$ 外其它元素为 $0$ 的的 $m \times 1$ 列向量，$W_2$ 是 $m \times p$ 矩阵，$W_1$ 是 $p \times n$ 矩阵， $\boldsymbol{x}$ 是 $n \times 1$ 列向量，$l$ 是标量； $\log$ 表示自然对数， $\operatorname{softmax}(\boldsymbol{a})=\dfrac{\exp (\boldsymbol{a})}{\mathbf{1}^T \exp (\boldsymbol{a})}$ 同上，$\sigma$ 是逐元素 $\operatorname{sigmoid}$ 函数 $\sigma(a)=\dfrac{1}{1+\exp (-a)}$ 。

**解：**

1. 定义 $\boldsymbol{a}_1=W_1 \boldsymbol{x}, \boldsymbol{h}_1=\sigma\left(\boldsymbol{a}_1\right), \boldsymbol{a}_2=W_2 \boldsymbol{h}_1$ ，则 $l=-\boldsymbol{y}^T \log \operatorname{softmax}\left(\boldsymbol{a}_2\right)$ 。在前例中已求出 $\dfrac{\partial l}{\partial a_2}=\operatorname{softmax}\left(\boldsymbol{a}_2\right)-\boldsymbol{y}$ 。

2. 使用复合法则
$$d l=\operatorname{tr}\left({\frac{\partial l}{\partial \boldsymbol{a}_2}}^T d \boldsymbol{a}_2\right)=\operatorname{tr}\left({\frac{\partial l}{\partial \boldsymbol{a}_2}}^T d W_2 \boldsymbol{h}_1\right)+\underbrace{\operatorname{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}{ }^T W_2 d \boldsymbol{h}_1\right)}_{d l_2}$$ 使用矩阵乘法交换的迹技巧从第一项得到 $\dfrac{\partial l}{\partial W_2}=\dfrac{\partial l}{\partial a_2} \boldsymbol{h}_1^T$ ，从第二项得到 $\dfrac{\partial l}{\partial \boldsymbol{h}_1}=W_2^T \dfrac{\partial l}{\partial \boldsymbol{a}_2}$ 。
3. 接下来对第二项继续使用复合法则来求 $\dfrac{\partial l}{\partial a_1}$ ，并利用矩阵乘法和逐元素乘法交换的迹技巧：
$$
\begin{align*}
dl_2&=\operatorname{tr}\left({\frac{\partial l}{\partial \boldsymbol{h}_1}}^T d \boldsymbol{h}_1\right)\\&=\operatorname{tr}\left({\frac{\partial l}{\partial \boldsymbol{h}_1}}^T\left(\sigma^{\prime}\left(\boldsymbol{a}_1\right) \odot d \boldsymbol{a}_1\right)\right)\\&=\operatorname{tr}\left(\left(\frac{\partial l}{\partial \boldsymbol{h}_1} \odot \sigma^{\prime}\left(\boldsymbol{a}_1\right)\right)^T d \boldsymbol{a}_1\right)
\end{align*}
$$
得到 $\dfrac{\partial l}{\partial \boldsymbol{a}_1}=\dfrac{\partial l}{\partial \boldsymbol{h}_1} \odot \sigma^{\prime}\left(\boldsymbol{a}_1\right)$ 。

> [!note]- 区分「矩阵（或向量）的乘法」和「逐元素相乘」
> 
> **符号含义不同**
> 
>    * “·”通常表示矩阵乘法（或向量的点积），如果写成
> $$
> \frac{\partial l}{\partial \boldsymbol{a}_1}
>  = \frac{\partial l}{\partial \boldsymbol{h}_1}
> \cdot \sigma'(\boldsymbol{a}_1),
> $$
> 它会被理解为一个向量（或行向量）乘以另一个向量，形状上根本对应不上（也不该做内积）。
>    * “⊙”（Hadamard 乘积）表示**逐元素相乘**，恰好对应了我们在反向传播中把每个神经元的误差信号与该神经元激活函数导数一一对应相乘的步骤。
> 
> **雅可比矩阵的结构**
>    对于逐元素激活函数 $\sigma$，有
> 
> $$
> \boldsymbol{h}_1 = \sigma(\boldsymbol{a}_1),\quad
> \color{red}{\frac{\partial \boldsymbol{h}_1}{\partial \boldsymbol{a}_1}
> = \operatorname{diag}\bigl(\sigma'(\boldsymbol{a}_1)\bigr)}
> $$
> 
>    这是一个对角矩阵。根据链式法则，
> 
> $$
> \frac{\partial l}{\partial \boldsymbol{a}_1}
> = \Bigl(\frac{\partial l}{\partial \boldsymbol{h}_1}\Bigr)\,
> \frac{\partial \boldsymbol{h}_1}{\partial \boldsymbol{a}_1}
> = \frac{\partial l}{\partial \boldsymbol{h}_1}\,
> \operatorname{diag}\bigl(\sigma'(\boldsymbol{a}_1)\bigr).
> $$
> 
>    用矩阵乘法展开，相当于
> 
> $$
> \Bigl(\frac{\partial l}{\partial \boldsymbol{a}_1}\Bigr)_i
> = \Bigl(\frac{\partial l}{\partial \boldsymbol{h}_1}\Bigr)_i
> \,\sigma'\bigl(\boldsymbol{a}_1\bigr)_i,
> $$
> 
>    也就是对每一个分量都做一次相乘——这正是逐元素乘积 (Hadamard 乘积)：
> $$
> \frac{\partial l}{\partial \boldsymbol{a}_1}
> = \frac{\partial l}{\partial \boldsymbol{h}_1}
> \;\odot\;\sigma'(\boldsymbol{a}_1).
> $$
> **形状匹配**
> 
>    * $\frac{\partial l}{\partial \boldsymbol{h}_1}$ 和 $\sigma'(\boldsymbol{a}_1)$ 都是同维度的列向量。
>    * 如果用矩阵乘法（·），列向量后面只能跟一个行向量或符合相乘规则的矩阵，根本不匹配。
>    * 用 “⊙” 就可以直接对应每个神经元信号：
> $$
> \bigl[\delta a_1\bigr]_i
> = \bigl[\delta h_1\bigr]_i \times \sigma'(a_{1,i}).
> $$
> 
> 总结：
> 
> > 在反向传播中，激活函数的导数与上一层的误差信号是一一对应的关系，既不是内积也不是标准的矩阵乘法，而是**逐元素相乘**（Hadamard 乘积），因此正确的写法是
> >$$
> \frac{\partial l}{\partial \boldsymbol{a}_1}
> = \frac{\partial l}{\partial \boldsymbol{h}_1}
> \;\odot\;\sigma'(\boldsymbol{a}_1).
> $$
> 

2. 为求 $\dfrac{\partial l}{\partial W_1}$ ，再用一次复合法则：
$$
\begin{align*}
d l_2 &= \operatorname{tr}\left({\left(\dfrac{\partial l}{\partial \boldsymbol{a}_1}\right)}^T d \boldsymbol{a}_1\right) \\
  &= \operatorname{tr}\left({\left(\dfrac{\partial l}{\partial \boldsymbol{a}_1}\right)}^T d W_1 \boldsymbol{x}\right) \\
  &= \operatorname{tr}\left(\boldsymbol{x}{\left(\dfrac{\partial l}{\partial \boldsymbol{a}_1}\right)}^T d W_1\right)
\end{align*}
$$ 
得到 $\dfrac{\partial l}{\partial W_1}=\dfrac{\partial l}{\partial \boldsymbol{a}_1} \boldsymbol{x}^T$

##### 推广 
样本 $\left(\boldsymbol{x}_1, y_1\right), \ldots,\left(\boldsymbol{x}_N, y_N\right)$ ， $l=-\sum_{i=1}^N \boldsymbol{y}_i^T \log \operatorname{softmax}\left(W_2 \sigma\left(W_1 \boldsymbol{x}_i+\boldsymbol{b}_1\right)+\boldsymbol{b}_2\right)$ ，其中 $\boldsymbol{b}_1$ 是 $p \times 1$ 列向量， $\boldsymbol{b}_2$ 是 $m \times 1$ 列向量，其余定义同上。

**解1**：

1. 定义 $\boldsymbol{a}_{1, i}=W_1 \boldsymbol{x}_i+\boldsymbol{b}_1, \boldsymbol{h}_{1, i}=\sigma\left(\boldsymbol{a}_{1, i}\right), \boldsymbol{a}_{2, i}=W_2 \boldsymbol{h}_{1, i}+\boldsymbol{b}_2$ ，则 $$l=-\sum_{i=1}^N \boldsymbol{y}_i^T \log \operatorname{softmax}\left(\boldsymbol{a}_{2, i}\right)$$
2. 先同上可求出 $\dfrac{\partial l}{\partial \boldsymbol{a}_{2, i}}=\operatorname{softmax}\left(\boldsymbol{a}_{2, i}\right)-\boldsymbol{y}_i$ 。使用复合法则
$$
\begin{align*}
 dl&=\operatorname{tr}\left(\left(\sum_{i=1}^N{\frac{\partial l}{\partial \boldsymbol{a}_{2, i}}}\right)^T d \boldsymbol{a}_{2, i}\right)\\&=\operatorname{tr}\left(\left(\sum_{i=1}^N{\frac{\partial l^{\prime}}{\partial \boldsymbol{a}_{2, i}}}\right)^T d W_2 \boldsymbol{h}_{1, i}\right)+\underbrace{\operatorname{tr}\left(\left(\sum_{i=1}^N{\frac{\partial l^{\prime}}{\partial \boldsymbol{a}_{2, i}}}\right)^T W_2 d \boldsymbol{h}_{1, i}\right)}_{d l_2}\\&+\operatorname{tr}\left(\left(\sum_{i=1}^N{\frac{\partial l^{\prime}}{\partial \boldsymbol{a}_{2, i}}}\right)^T d \boldsymbol{b}_2\right)
\end{align*}
$$
3. 第一项得到得到 $\dfrac{\partial l}{\partial W_2}=\sum_{i=1}^N \dfrac{\partial l}{\partial \boldsymbol{a}_{2, i}} \boldsymbol{h}_{1, i}^T$ ，从第二项得到 $\dfrac{\partial l}{\partial \boldsymbol{h}_{1, i}}=W_2^T \dfrac{\partial l}{\partial \boldsymbol{a}_{2, i}}$ ，从第三项得到到 $\dfrac{\partial l}{\partial b_2}=\sum_{i=1}^N \dfrac{\partial l}{\partial a_{2, i}}$ 。接下来对第二项继续使用复合法则，得到 $\dfrac{\partial l}{\partial \boldsymbol{a}_{1, i}}=\dfrac{\partial l}{\partial \boldsymbol{h}_{1, i}} \odot \sigma^{\prime}\left(\boldsymbol{a}_{1, i}\right)$ 。为求 $\dfrac{\partial l}{\partial W_1}, \dfrac{\partial l}{\partial \boldsymbol{b}_1}$ ，再用一次复合法则：

$$
\begin{align*}
dl_2&=\operatorname{tr}\left(\left(\sum_{i=1}^N{\frac{\partial l}{\partial \boldsymbol{a}_{1, i}}}\right)^T d \boldsymbol{a}_{1, i}\right)\\&=\operatorname{tr}\left(\left(\sum_{i=1}^N{\frac{\partial l}{\partial \boldsymbol{a}_{1, i}}}\right)^T d W_1 \boldsymbol{x}_i\right)+\operatorname{tr}\left(\left(\sum_{i=1}^N{\frac{\partial l}{\partial \boldsymbol{a}_{1, i}}}\right)^T d \boldsymbol{b}_1\right)\end{align*}
$$

4. 得到 $\dfrac{\partial l}{\partial W_1}=\sum_{i=1}^N \dfrac{\partial l}{\partial \boldsymbol{a}_{1, i}} \boldsymbol{x}_i^T, \dfrac{\partial l}{\partial \boldsymbol{b}_1}=\sum_{i=1}^N \dfrac{\partial l}{\partial \boldsymbol{a}_{1, i}}$ 

$$\begin{align*}
\frac{\partial f}{\partial X} &= W^T\left(\frac{\partial f}{\partial Y} \odot \sigma^{\prime}(W X)\right) \\
  &= W^T\left((2 M \sigma(W X)) \odot \sigma^{\prime}(W X)\right)
\end{align*}$$

**解 2**：
1. 可以用矩阵来表示 $N$ 个样本，以简化形式。定义 
$$
\begin{align*}X&=\left[\boldsymbol{x}_1, \cdots, \boldsymbol{x}_N\right]\\A_1&=\left[\boldsymbol{a}_{1,1}, \cdots, \boldsymbol{a}_{1, N}\right]=W_1 X+\boldsymbol{b}_1 \mathbf{1}^T\\H_1&=\left[\boldsymbol{h}_{1,1}, \cdots, \boldsymbol{h}_{1, N}\right]=\sigma\left(A_1\right)\\A_2&=\left[\boldsymbol{a}_{2,1}, \cdots, \boldsymbol{a}_{2, N}\right]=W_2 H_1+\boldsymbol{b}_2 \mathbf{1}^T\end{align*}
$$
注意这里使用全 1 向量来扩展维度。
2. 先同上求出 $\dfrac{\partial l}{\partial A_2}=\left[\operatorname{softmax}\left(\boldsymbol{a}_{2,1}\right)-\boldsymbol{y}_1, \cdots, \operatorname{softmax}\left(\boldsymbol{a}_{2, N}\right)-\boldsymbol{y}_N\right]$ 。使用复合法则，
$$
\begin{align*}
d l &= \operatorname{tr}\left({\frac{\partial l}{\partial A_2}}^T d A_2\right) \\
  &= \operatorname{tr}\left({\frac{\partial l}{\partial A_2}}^T d W_2 H_1\right)+\underbrace{\operatorname{tr}\left({\frac{\partial l}{\partial A_2}}^T W_2 d H_1\right)}_{d l_2}+\operatorname{tr}\left({\frac{\partial l}{\partial A_2}}^T d \boldsymbol{b}_2 \mathbf{1}^T\right)
\end{align*}
$$
3. 从第一项得到 $\dfrac{\partial l}{\partial W_2}=\dfrac{\partial l}{\partial A_2} H_1^T$ ，从第二项得到 $\dfrac{\partial l}{\partial H_1}=W_2^T \dfrac{\partial l}{\partial A_2}$ ，从第三项得到到 $\dfrac{\partial l}{\partial b_2}=\dfrac{\partial l}{\partial A_2} \mathbf{1}$ 。
4. 接下来对第二项继续使用复合法则，得到 $\dfrac{\partial l}{\partial A_1}=\dfrac{\partial l}{\partial H_1} \odot \sigma^{\prime}\left(A_1\right)$ 。
5. 为求 $\dfrac{\partial l}{\partial W_1}, \dfrac{\partial l}{\partial b_1}$ ，再用一次复合法则：
$$
d l_2=\operatorname{tr}\left({\dfrac{\partial l}{\partial A_1}}^T d A_1\right)=\operatorname{tr}\left({\dfrac{\partial l}{\partial A_1}}^T d W_1 X\right)+\operatorname{tr}\left({\dfrac{\partial l}{\partial A_1}}^T d \boldsymbol{b}_1 \mathbf{1}^T\right)
$$
得到 $\dfrac{\partial l}{\partial W_1}=\dfrac{\partial l}{\partial A_1} X^T, \quad \dfrac{\partial l}{\partial b_1}=\dfrac{\partial l}{\partial A_1} \mathbf{1}_{\circ}$


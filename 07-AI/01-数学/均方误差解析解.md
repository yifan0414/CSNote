---
created: 2025-04-01 17:15
tags:
---
我们将通过矩阵和向量表示法来推导出**线性回归优化问题的解析解**。在此过程中，我们将忽略偏置项 $b$，并通过向数据矩阵 $\mathbf{X}$ 添加一列常数为 1 的值来实现这一点（即将偏置项视为模型的参数之一）。具体步骤如下：

### 1. 优化问题的矩阵和向量表示法

我们考虑一个线性回归问题，其中目标是通过一组输入数据 $\mathbf{X}$ 来预测目标变量 $\mathbf{y}$。在简化问题时，我们可以忽略偏置项，并假设数据矩阵 $\mathbf{X}$ 已经包含了额外的一列常数 1，来代替偏置项。

- 设数据矩阵 $\mathbf{X} \in \mathbb{R}^{n \times m}$，其中 $n$ 是样本数量，$m$ 是特征数量。每一行是一个样本，包含 $m$ 个特征。
- 目标向量 $\mathbf{y} \in \mathbb{R}^{n}$ 是对应于每个样本的真实标签。

线性回归的模型为：

$$
\mathbf{\hat{y}} = \mathbf{X} \mathbf{w},
$$

其中，$\mathbf{w} \in \mathbb{R}^m$ 是待估计的权重向量。

目标是最小化**平方误差损失函数**：

$$
L(\mathbf{w}) = \frac{1}{2} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2 = \frac{1}{2} \|\mathbf{y} - \mathbf{X} \mathbf{w}\|^2,
$$

其中 $\|\mathbf{y} - \mathbf{X} \mathbf{w}\|^2$ 是目标值和预测值之间的欧几里得距离的平方。

### 2. 计算损失对 $\mathbf{w}$ 的梯度

为了最小化损失函数 $L(\mathbf{w})$，我们需要计算损失函数关于 $\mathbf{w}$ 的梯度。首先，展开损失函数：

$$
L(\mathbf{w}) = \frac{1}{2} \|\mathbf{y} - \mathbf{X} \mathbf{w}\|^2 = \frac{1}{2} (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w}).
$$

接下来，对 $\mathbf{w}$ 求梯度：

$$
\frac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = \frac{\partial}{\partial \mathbf{w}} \left( \frac{1}{2} (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w}) \right).
$$

使用链式法则，首先对平方项求导：

$$
\frac{\partial}{\partial \mathbf{w}} \left( (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w}) \right) = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \mathbf{w}).
$$
好的，让我们详细推导一下这个梯度计算过程。

我们需要对以下表达式求梯度：

$$
L(\mathbf{w}) = (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w}).
$$

> [!NOTE] 推导步骤
> ### 步骤 1: 展开损失函数
> 
> 首先，我们可以将该表达式展开：
> 
> $$
> L(\mathbf{w}) = (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w}).
> $$
> 
> 这是一个向量的内积，展开它得到：
> 
> $$
> L(\mathbf{w}) = \mathbf{y}^\top \mathbf{y} - \mathbf{y}^\top \mathbf{X} \mathbf{w} - (\mathbf{X} \mathbf{w})^\top \mathbf{y} + (\mathbf{X} \mathbf{w})^\top (\mathbf{X} \mathbf{w}).
> $$
> 
> 其中 $(\mathbf{X} \mathbf{w})^\top \mathbf{y}$ 是标量，它等于 $\mathbf{y}^\top \mathbf{X} \mathbf{w}$，所以我们可以进一步简化为：
> 
> $$
> L(\mathbf{w}) = \mathbf{y}^\top \mathbf{y} - 2 \mathbf{y}^\top \mathbf{X} \mathbf{w} + \mathbf{w}^\top \mathbf{X}^\top \mathbf{X} \mathbf{w}.
> $$
> 
> ### 步骤 2: 求梯度
> 
> 接下来，我们对 $L(\mathbf{w})$ 关于 $\mathbf{w}$ 求导。我们分别对每一项求导：
> 
> 1. 对于第一项 $\mathbf{y}^\top \mathbf{y}$，它与 $\mathbf{w}$ 无关，所以它的导数是零。
>    
> 2. 对于第二项 $-2 \mathbf{y}^\top \mathbf{X} \mathbf{w}$，我们使用常规的向量求导规则。注意到 $\mathbf{y}^\top \mathbf{X} \mathbf{w}$ 是一个标量，导数是：
> 
>    $$
>    \frac{\partial}{\partial \mathbf{w}} \left( -2 \mathbf{y}^\top \mathbf{X} \mathbf{w} \right) = -2 \mathbf{X}^\top \mathbf{y}.
> $$
> 
> 3. 对于第三项 $\mathbf{w}^\top \mathbf{X}^\top \mathbf{X} \mathbf{w}$，它是一个二次形式。根据二次型求导规则，我们知道：
> 
>    $$
>    \frac{\partial}{\partial \mathbf{w}} \left( \mathbf{w}^\top \mathbf{X}^\top \mathbf{X} \mathbf{w} \right) = 2 \mathbf{X}^\top \mathbf{X} \mathbf{w}.
> $$
> 
> ### 步骤 3: 汇总梯度
> 
> 将这三项求导结果结合起来，我们得到损失函数对 $\mathbf{w}$ 的梯度：
> 
> $$
> \frac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = -2 \mathbf{X}^\top \mathbf{y} + 2 \mathbf{X}^\top \mathbf{X} \mathbf{w}.
> $$
> 
> 简化后，得到：
> 
> $$
> \frac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = 2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}).
> $$
> 
> 通常，我们将这个梯度除以 2，以便消除前面的系数 2（这在最小化过程中并不影响结果）。因此，最终的梯度为：
> 
> $$
> \frac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}).
> $$
> 
> 这个结果即是损失函数对权重 $\mathbf{w}$ 的梯度。
> 
> ### 步骤 4: 最终梯度与原始表达式的比较
> 
> 我们最初的目标是计算：
> 
> $$
> \frac{\partial}{\partial \mathbf{w}} \left( (\mathbf{y} - \mathbf{X} \mathbf{w})^\top (\mathbf{y} - \mathbf{X} \mathbf{w}) \right).
> $$
> 
> 最终的梯度表达式为：
> 
> $$
> -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \mathbf{w}),
> $$
> 
> 与我们刚刚推导出的梯度结果一致，只是我们省略了常数因子 2，因此最终的梯度结果是：
> 
> $$
> \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}),
> $$
> 
> 这就是损失函数的梯度。

因此，梯度为：

$$
\frac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = - \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \mathbf{w}).
$$

### 3. 通过将梯度设为 0 来求解解析解

为了找到最优的 $\mathbf{w}$，我们将梯度设为零：

$$
\mathbf{X}^\top (\mathbf{y} - \mathbf{X} \mathbf{w}) = 0.
$$

展开：

$$
\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \mathbf{w} = 0.
$$

解得：

$$
\color{red} \mathbf{w} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}.
$$

这个解就是线性回归的**解析解**，也称为最小二乘解。

### 4. 何时解析解比随机梯度下降更好？何时会失效？

#### 解析解的优点

- **计算量较小**：如果数据集较小或者特征维度不高，计算 $(\mathbf{X}^\top \mathbf{X})^{-1}$ 是非常直接且高效的。解析解给出了精确解，而不需要迭代。
- **收敛速度快**：解析解是全局最优解，直接给出，不需要通过反复的迭代过程（如随机梯度下降）来逼近最优解。
- **精确性**：解析解能够提供精确的最优权重，不像随机梯度下降那样可能会受初始化和学习率等因素影响。

#### 解析解的缺点与失效情况

- **计算量大**：当数据集很大（即 $n$ 或 $m$ 很大）时，计算矩阵 $\mathbf{X}^\top \mathbf{X}$ 的逆可能非常耗时。特别是当特征维度 $m$ 非常高时，求逆的计算复杂度为 $O(m^3)$。
- **数值稳定性问题**：如果 $\mathbf{X}^\top \mathbf{X}$ 是**奇异的**或者**接近奇异**（即其行列式接近零），那么求解 $(\mathbf{X}^\top \mathbf{X})^{-1}$ 会导致数值不稳定，可能会出现计算误差或者无法计算出逆矩阵。
- **内存消耗**：对于非常大的数据集，存储和操作矩阵 $\mathbf{X}^\top \mathbf{X}$ 可能需要大量的内存。

#### 随机梯度下降的优势

- **计算效率**：对于大规模数据集，随机梯度下降（SGD）比求解解析解更高效，因为它不需要计算矩阵的逆。每次更新只需要使用一个小批量的数据，计算成本低。
- **更适合高维数据**：当特征数 $m$ 很大时，解析解的计算变得不切实际，而随机梯度下降能够高效地处理高维数据。

#### 随机梯度下降的缺点

- **需要选择学习率**：SGD 的收敛速度依赖于学习率的选择。过大的学习率可能导致发散，过小的学习率则可能导致收敛过慢。
- **需要多次迭代**：SGD 是一个迭代过程，在每一次迭代中只处理一小批数据，因此需要多个迭代才能收敛。

### 总结

- 解析解适合小规模数据集或者特征数较少的情况，它能够提供精确的解，但当数据集非常大时，计算和内存开销会成为瓶颈。
- 随机梯度下降适合大规模数据集，特别是当数据量非常大或者特征数很多时，它能够高效地进行参数更新，但它依赖于合适的学习率，并且只能找到一个局部最优解。

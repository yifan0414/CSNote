---
created: 2025-04-11 15:52
tags:
---
交叉熵函数（Cross-Entropy Loss）的梯度是优化神经网络（尤其是分类问题中的逻辑回归或神经网络）时使用的关键内容。交叉熵损失函数常用于分类任务中，它衡量了预测概率分布与实际标签之间的差异。

我们可以分别推导 **二分类** 和 **多分类** 问题中交叉熵损失函数的梯度。

### 1. **二分类问题中的交叉熵梯度**
在二分类问题中，假设模型输出通过 **sigmoid 函数** $f(x) = \sigma(z)$ 计算，其中 $\sigma(z) = \frac{1}{1 + e^{-z}}$ 是sigmoid函数，$z$ 是模型的线性输出（例如 $z = \mathbf{w}^T \mathbf{x} + b$）。

交叉熵损失函数的形式是：
$$
L(f) = - \sum_n \left[ \hat{y}^n \log(f(x^n)) + (1 - \hat{y}^n) \log(1 - f(x^n)) \right]
$$
其中：
- $\hat{y}^n$ 是真实标签（0 或 1），
- $f(x^n) = \sigma(z^n)$ 是模型预测的概率。

交叉熵损失函数的梯度计算如下：

对于每个数据点 $n$，我们首先计算损失函数对模型输出 $f(x^n)$ 的梯度：
$$
\frac{\partial L}{\partial f(x^n)} = - \left( \frac{\hat{y}^n}{f(x^n)} - \frac{1 - \hat{y}^n}{1 - f(x^n)} \right)
$$

然后，我们计算损失函数对模型输入 $z^n$ 的梯度。由于 $f(x^n) = \sigma(z^n)$，我们需要应用链式法则来计算：
$$
\frac{\partial L}{\partial z^n} = \frac{\partial L}{\partial f(x^n)} \cdot \frac{\partial f(x^n)}{\partial z^n}
$$
由于 $f(x^n) = \sigma(z^n)$，我们有：
$$
\frac{\partial f(x^n)}{\partial z^n} = f(x^n) \cdot (1 - f(x^n))
$$

因此：
$$
\frac{\partial L}{\partial z^n} = f(x^n) - \hat{y}^n
$$

最终，参数（权重 $w_i$ 和偏置 $b$）的梯度可以通过以下公式来更新：
$$
\frac{\partial L}{\partial w_i} = (f(x^n) - \hat{y}^n) \cdot x_i^n
$$
$$
\frac{\partial L}{\partial b} = f(x^n) - \hat{y}^n
$$
这些梯度将用于梯度下降算法中，调整模型的权重和偏置，最小化交叉熵损失。

### 2. **多分类问题中的交叉熵梯度**
在多分类问题中，我们使用 **softmax 函数** 来将网络的输出转换为概率分布，softmax函数的输出 $\hat{y}_i^n$ 表示数据点 $n$ 属于类别 $i$ 的概率。交叉熵损失函数的形式为：
$$
L(f) = - \sum_n \sum_{i=1}^{C} \hat{y}_i^n \log(f_i(x^n))
$$
其中：
- $C$ 是类别数，
- $\hat{y}_i^n$ 是真实标签的one-hot编码（对于类别 $i$ 的标签，如果属于该类别则为 1，否则为 0），
- $f_i(x^n)$ 是模型对类别 $i$ 的预测概率，通过softmax函数计算。

softmax函数的形式是：
$$
f_i(x^n) = \frac{e^{z_i^n}}{\sum_{k=1}^{C} e^{z_k^n}}
$$
其中 $z_i^n$ 是类别 $i$ 对应的线性输出。

交叉熵损失函数对 $z_i^n$ 的梯度为：
$$
\frac{\partial L}{\partial z_i^n} = f_i(x^n) - \hat{y}_i^n
$$
这里，梯度仅取决于预测概率和真实标签之间的差异。

对于每个类别 $i$，权重 $w_i$ 和偏置 $b$ 的梯度为：
$$
\frac{\partial L}{\partial w_i} = \sum_n (f_i(x^n) - \hat{y}_i^n) \cdot x_i^n
$$
$$
\frac{\partial L}{\partial b} = \sum_n (f_i(x^n) - \hat{y}_i^n)
$$
这些梯度也将用于梯度下降算法中，调整模型的参数。

### 总结
- **二分类问题** 中，交叉熵损失的梯度是 $f(x^n) - \hat{y}^n$，其中 $f(x^n)$ 是通过 sigmoid 激活函数计算得到的概率。
- **多分类问题** 中，交叉熵损失的梯度是 $f_i(x^n) - \hat{y}_i^n$，其中 $f_i(x^n)$ 是通过 softmax 激活函数计算得到的每个类别的预测概率。

交叉熵损失函数的梯度简单且易于计算，这使得它在分类问题中非常有效。它通过衡量模型预测概率与真实标签之间的差异，直接引导梯度下降算法优化模型参数。
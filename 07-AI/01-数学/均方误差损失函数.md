---
created: 2025-04-01 17:03
tags:
---
你提到的这一段话涉及了**线性回归**和**极大似然估计**的概念。我们可以从概率和统计的角度来解释为什么均方误差损失函数与噪声服从正态分布有关系。

### 1. **噪声服从正态分布的假设**

在实际的回归问题中，观测到的目标值 $y^{(i)}$ 是由输入特征 $\mathbf{x}^{(i)}$ 经过某个线性模型预测得到的，即 $\mathbf{w}^\top \mathbf{x}^{(i)} + b$，但是因为数据中往往存在一些噪声，因此我们假设噪声 $\epsilon$ 服从正态分布。具体来说，观测值 $y^{(i)}$ 可以表示为：

$$
y^{(i)} = \mathbf{w}^\top \mathbf{x}^{(i)} + b + \epsilon^{(i)},
$$

其中 $\epsilon^{(i)}$ 是噪声，服从均值为 0、方差为 $\sigma^2$ 的正态分布，即：

$$
\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^2).
$$

这意味着观测到的 $y^{(i)}$ 实际上是模型的预测值 $\mathbf{w}^\top \mathbf{x}^{(i)} + b$ 加上一些随机波动（噪声）。我们假设这个噪声是由正态分布产生的，且不同的观测之间的噪声是独立的。

### 2. **似然函数**

给定输入特征 $\mathbf{x}^{(i)}$ 和目标值 $y^{(i)}$，我们希望通过似然函数来描述观测到 $y^{(i)}$ 的概率。我们假设噪声是正态分布的，所以条件概率 $P(y^{(i)} \mid \mathbf{x}^{(i)})$ 也遵循正态分布：

$$
P(y^{(i)} \mid \mathbf{x}^{(i)}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b)^2\right).
$$

这就是给定输入特征 $\mathbf{x}^{(i)}$，观测到 $y^{(i)}$ 的概率密度函数。这里：

- $\mathbf{w}^\top \mathbf{x}^{(i)} + b$ 是模型的预测值，
- $(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b)$ 是误差项，即模型预测和实际观测值之间的差距，
- 正态分布的参数 $\sigma^2$ 是噪声的方差。

### 3. **极大似然估计**

极大似然估计（Maximum Likelihood Estimation, MLE）是一种估计模型参数的方法，它的目标是找到一组参数，使得观测数据出现的概率最大化。我们要求解的就是使得整个数据集的似然最大化的参数 $\mathbf{w}$ 和 $b$。

因此，整个数据集的似然函数是每个观测的似然函数的乘积：

$$
P(\mathbf{y} \mid \mathbf{X}) = \prod_{i=1}^{n} P(y^{(i)} \mid \mathbf{x}^{(i)}).
$$

这就是数据集的总似然函数。为了最大化似然，我们通常取对数似然，因为对数函数是单调递增的，且可以将乘积转换为和，简化计算。对数似然函数为：

$$
\log P(\mathbf{y} \mid \mathbf{X}) = \sum_{i=1}^{n} \log P(y^{(i)} \mid \mathbf{x}^{(i)}).
$$

代入之前的似然公式，得到：

$$
\log P(\mathbf{y} \mid \mathbf{X}) = -\frac{n}{2} \log (2 \pi \sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^{n} \left( y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b \right)^2.
$$

最大化对数似然等价于最小化平方误差，因为 $\frac{1}{2 \sigma^2}$ 和 $\log (2 \pi \sigma^2)$ 是常数，可以忽略。因此，最大化似然函数实际上就是最小化下面的目标函数：

$$
\sum_{i=1}^{n} \left( y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b \right)^2.
$$

这个目标函数正是 **均方误差**（MSE）损失函数，因此我们得出结论：如果假设噪声服从正态分布，那么最优的模型参数就是使得均方误差损失函数最小化的参数。

### 4. **总结**

- 通过假设噪声 $\epsilon^{(i)}$ 服从正态分布 $\mathcal{N}(0, \sigma^2)$，我们可以推导出每个观测值的条件概率（即似然）。
- 极大似然估计（MLE）方法要求我们找到使得整个数据集的似然最大化的参数。
- 最大化似然函数等价于最小化均方误差（MSE），这就是为什么均方误差损失函数可以用于线性回归的原因。

## 习题
> [!math]
> 假设我们有一些数据 $x_1, \ldots, x_n \in \mathbb{R}$。我们的目标是找到一个常数 $b$，使得最小化 $\sum_i (x_i - b)^2$。
> 	1. 找到最优值 $b$ 的解析解。
> 	2.  这个问题及其解与正态分布有什么关系?


### 1. 最小化目标函数

我们有一些数据 $x_1, x_2, \dots, x_n \in \mathbb{R}$，并且我们的目标是找到一个常数 $b$，使得以下的目标函数最小化：

$$
\sum_{i=1}^{n} (x_i - b)^2.
$$

这是一个经典的最小二乘问题，我们的目标是通过调整常数 $b$ 来最小化每个数据点 $x_i$ 与 $b$ 之间的平方差。

#### 求解最优值 $b$

为了找到 $b$ 的最优值，我们对目标函数求导并令其等于零。首先，展开目标函数：

$$
\sum_{i=1}^{n} (x_i - b)^2 = \sum_{i=1}^{n} (x_i^2 - 2x_i b + b^2).
$$

我们对 $b$ 求导数：

$$
\frac{d}{db} \sum_{i=1}^{n} (x_i^2 - 2x_i b + b^2) = \frac{d}{db} \sum_{i=1}^{n} x_i^2 - \frac{d}{db} \sum_{i=1}^{n} 2x_i b + \frac{d}{db} \sum_{i=1}^{n} b^2.
$$

注意到前两项中和 $b$ 无关，因此它们的导数为零，剩下的是：

$$
-2 \sum_{i=1}^{n} x_i + 2nb.
$$

令其等于零以找到最优解：

$$
-2 \sum_{i=1}^{n} x_i + 2nb = 0.
$$

解得：

$$
b = \frac{1}{n} \sum_{i=1}^{n} x_i.
$$

因此，最优值 $b$ 是数据点的**平均值**。

### 2. 这个问题与正态分布的关系

这个最小化问题与正态分布有很密切的关系。假设我们观察到的每个 $x_i$ 都是由一个真实的常数 $\mu$ 加上一些独立的噪声 $\epsilon_i$ 得到的，其中噪声服从正态分布，即：

$$
x_i = \mu + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2).
$$

那么，给定数据点 $x_1, x_2, \dots, x_n$，我们实际上是在估计真实值 $\mu$，而噪声 $\epsilon_i$ 的分布是正态的。目标函数 $\sum_{i=1}^{n} (x_i - b)^2$ 可以看作是根据观测值 $x_i$ 对 $\mu$ 的**最大似然估计**。

#### 关系的具体推导

根据极大似然估计的原理，假设我们观察到的数据 $x_1, x_2, \dots, x_n$ 是从正态分布中独立采样的，即每个 $x_i$ 都服从 $\mathcal{N}(\mu, \sigma^2)$。则似然函数 $P(x_1, x_2, \dots, x_n \mid \mu)$ 可以写成：

$$
P(x_1, x_2, \dots, x_n \mid \mu) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(x_i - \mu)^2}{2 \sigma^2} \right).
$$

取对数得到对数似然函数：

$$
\log P(x_1, x_2, \dots, x_n \mid \mu) = -\frac{n}{2} \log(2\pi \sigma^2) - \frac{1}{2 \sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2.
$$

为了最大化对数似然，我们最大化第二项（因为其他项与 $\mu$ 无关）：

$$
\sum_{i=1}^{n} (x_i - \mu)^2.
$$

这就是我们在最小化的目标函数。因此，最大化似然等价于最小化平方误差，而最小化平方误差的最优解就是 $\mu$ 的样本均值，即：

$$
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i.
$$

所以，最优的 $b$（即最大似然估计值）就是数据的均值，而这个结果表明，**最小化平方误差相当于最大化正态分布的似然函数**，也就是说，均值是正态分布的最大似然估计。

> [!note]
> - **最小化平方误差**：是从“几何距离”角度优化模型。
> - **最大化似然**：是从“概率”角度优化模型。
> - **当误差正态分布时，两者完全等价**。

### 总结

1. 通过最小化 $\sum_{i=1}^{n} (x_i - b)^2$，我们得到了最优值 $b = \frac{1}{n} \sum_{i=1}^{n} x_i$，即数据的均值。
2. 这个问题的解与正态分布密切相关。实际上，如果数据 $x_1, x_2, \dots, x_n$ 是由正态分布生成的，那么最小化平方误差的目标函数就是最大化正态分布的似然函数，而均值就是正态分布的最大似然估计。
---
created: 2025-04-11 20:14
tags:
---
**问题**: 我们要证明 **多层感知机（MLP）** 在使用了激活函数的情况下 **不能退化为线性模型**。

**结论**: 如果一个多层感知机（MLP）包含至少一个激活函数，并且该激活函数是非线性的，那么它不能退化成线性模型，即使它包含多个层。

### 证明思路

我们首先需要理解一个关键的数学概念，即 **线性变换**。如果一个模型能够表示为 **线性模型**，那么它必须是通过一系列 **线性变换**（即加权和）得到的。

### 1. **线性模型**
一个线性模型可以表示为：
$$
\hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$
其中，$w_1, w_2, \dots, w_n$ 是权重，$x_1, x_2, \dots, x_n$ 是输入特征，$b$ 是偏置项。

这是一个 **线性函数**，即输入特征通过加权求和来获得输出。

### 2. **多层感知机（MLP）**
多层感知机的结构是由多层神经元（每层都有激活函数）组成的。假设我们有一个包含两层的MLP：
- 第一层是线性变换（加权和），然后通过激活函数（如sigmoid、ReLU等）。
- 第二层也是线性变换。

假设第一层的输入是 $\mathbf{x}$（可以是原始输入或者上一层的输出），权重矩阵为 $W_1$，偏置为 $b_1$，激活函数为 $f$（如 ReLU 或 Sigmoid）：
$$
h_1 = f(W_1 \mathbf{x} + b_1)
$$
第二层的输出是：
$$
\hat{y} = W_2 h_1 + b_2
$$

### 3. **如果没有激活函数，退化为线性模型**
如果没有激活函数 $f$（即 $f$ 是恒等函数），那么第一层的输出就是：
$$
h_1 = W_1 \mathbf{x} + b_1
$$
此时，第二层的输出为：
$$
\hat{y} = W_2 h_1 + b_2 = W_2 (W_1 \mathbf{x} + b_1) + b_2 = (W_2 W_1) \mathbf{x} + (W_2 b_1 + b_2)
$$
这就是一个线性模型，它是对输入 $\mathbf{x}$ 的线性变换。

### 4. **引入激活函数**
当激活函数 $f$ 是非线性时（如 ReLU 或 Sigmoid），第一层的输出就变为非线性函数的结果：
$$
h_1 = f(W_1 \mathbf{x} + b_1)
$$
此时，第二层的输出变为：
$$
\hat{y} = W_2 h_1 + b_2 = W_2 f(W_1 \mathbf{x} + b_1) + b_2
$$
由于 $f$ 是非线性函数，整个网络的输出是非线性的。即使有多个层，这个网络依然是非线性的。

### 5. **非线性激活函数的作用**
激活函数的作用是引入非线性，使得每一层的输出不仅仅是输入的线性组合。如果没有激活函数，网络的所有层加起来仍然是一个线性变换，无法捕捉到复杂的模式和非线性关系。因此，如果每一层都使用线性变换，而没有激活函数，那么无论层数多深，整个网络的输出依然是一个线性函数，无法表示更复杂的模式。

### 6. **结论**
通过上述推导，我们可以看出，**当多层感知机中的每一层都没有激活函数时，它等价于一个线性模型**，即使层数增加，模型也只能表示线性关系。而如果引入 **非线性激活函数**（如ReLU、Sigmoid等），则网络的每一层都会引入非线性，最终使得整个网络能够表示复杂的非线性关系，因此 **多层感知机就不能退化为线性模型**。

### 总结：
有了激活函数后，多层感知机能够表示 **非线性函数**，这使得它不同于线性模型，能够处理更为复杂的任务。激活函数的引入是让神经网络模型具有表达复杂关系的关键因素。


#  仿射函数的仿射函数本身就是仿射函数， 但是我们之前的线性模型已经能够表示任何仿射函数。 

这句话包含了对 **仿射函数**（Affine Function）和 **线性模型** 之间关系的一个重要理解。让我们逐步解析这个问题，理解每个部分的含义。

### 1. **仿射函数的定义**
   **仿射函数** 是指一种函数，它是 **线性函数** 加上一个常数项。通常，它的形式可以表示为：
   $$
   f(\mathbf{x}) = A\mathbf{x} + b
$$
   其中：
   - $\mathbf{x}$ 是输入向量（可以是多维的），
   - $A$ 是线性变换矩阵，
   - $b$ 是偏置（常数项）。

   也就是说，仿射函数是将输入通过一个线性变换（如矩阵乘法）后，再加上一个常数项（偏置）得到输出。**仿射函数**的核心特点是：它包含了一个线性变换（矩阵乘法）和一个偏置项。

### 2. **线性模型**
   线性模型通常是指一种通过加权和来预测输出的模型，公式为：
   $$
   \hat{y} = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$
   这其实是一个仿射函数，因为它的形式可以写成：
   $$
   \hat{y} = W \mathbf{x} + b
$$
   其中 $W$ 是权重向量，$\mathbf{x}$ 是输入特征向量，$b$ 是偏置。

   所以，**线性模型本身就是一个仿射函数**，即它可以通过仿射变换来表示。这个模型本质上对输入进行加权求和后加上一个常数（偏置项），因此它与仿射函数的定义是相同的。

### 3. **“仿射函数的仿射函数本身就是仿射函数”**
   这句话的意思是，如果你将一个仿射函数再通过仿射变换处理一次，它依然是一个仿射函数。换句话说，仿射变换的组合仍然会是一个仿射变换。

   - 假设我们有两个仿射函数：
     $$
     f_1(\mathbf{x}) = A\mathbf{x} + b
$$
     和
     $$
     f_2(\mathbf{y}) = C\mathbf{y} + d
$$
     如果我们把第一个函数的输出 $f_1(\mathbf{x})$ 作为第二个函数的输入，即：
     $$
     f_2(f_1(\mathbf{x})) = C(A\mathbf{x} + b) + d
$$
     可以化简为：
     $$
     f_2(f_1(\mathbf{x})) = (CA)\mathbf{x} + (Cb + d)
$$
     这仍然是一个仿射函数，因为它仍然是一个 **线性变换**（矩阵乘法）加一个偏置项。因此，两个仿射函数的组合，依然是一个仿射函数。

### 4. **“我们之前的线性模型已经能够表示任何仿射函数”**
   这一点是强调 **线性模型的表达能力**。线性模型（或仿射模型）实际上可以表示任何仿射变换，因为它本身就是一个仿射函数。**线性模型能够表示所有的仿射函数**，这意味着你不需要额外的工具来表示仿射变换。

   例如，在线性回归中，我们通过线性模型来拟合数据，实际上我们是在通过一个仿射函数来表示输入与输出之间的关系。无论是从数据的加权和（线性部分）还是偏置（常数项）来看，线性回归都能捕捉到仿射变换的所有信息。

### 总结：
- **仿射函数** 是线性变换加上一个常数项。
- **线性模型** 本质上就是一个仿射函数，表示为加权和加上偏置项。
- **仿射函数的仿射函数** 依然是仿射函数，因为两个仿射变换的组合还是仿射变换。
- 因此，**线性模型已经能够表示任何仿射函数**，没有必要额外的复杂结构去表示仿射变换，因为它已经包含了仿射函数的特性。

这就解释了为什么“线性模型已经能够表示任何仿射函数”，并且“仿射函数的仿射函数本身仍然是仿射函数”。
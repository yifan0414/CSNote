---
created: 2025-04-02 09:18
tags:
---
信息论中的交叉熵公式是通过衡量信息量、熵和概率分布之间的关系得出的。我们可以从以下几个角度来理解和推导交叉熵公式。

### 1. 熵（Entropy）与信息量（Information Content）

在信息论中，**熵**是衡量一个随机变量的不确定性的度量。对于一个离散的随机变量 $X$（取值为 $x_1, x_2, ..., x_n$），其熵定义为：

$$
H(X) = - \sum_{i} p(x_i) \log p(x_i)
$$

这里：
- $p(x_i)$ 是随机变量 $X$ 取值为 $x_i$ 的概率。
- $H(X)$ 表示对这个随机变量 $X$ 的不确定性的量化。

熵本质上表示了平均信息量，换句话说，它告诉我们在给定某个分布的情况下，需要多少比特的信息才能编码该分布的每个事件。

> [!hint]
> 
> 在分类问题中，真实标签的概率分布 $p(y)$ 被编码为事件的发生概率。交叉熵度量了在用预测分布 $q$ 进行编码时所需要的“额外”信息量（即惊异）。最小化交叉熵实际上是使得编码标签所需的额外信息最小，从而实现高效的标签传递。

### 2. 交叉熵（Cross-Entropy）

**交叉熵**是衡量两个概率分布之间差异的一种方式。它是一个分布 $p$ 和一个估计分布 $q$ 之间的距离度量，定义为：

$$
H(p, q) = - \sum_{i} p(x_i) \log q(x_i)
$$

其中：
- $p(x_i)$ 是真实分布 $p$ 的概率。
- $q(x_i)$ 是估计分布 $q$ 的概率。**所以 $\log q(x_{i})$ 代表了估计分布的信息量**

交叉熵可以理解为使用估计分布 $q(x)$ 来表示真实分布 $p(x)$ 时，所需要的编码长度（单位是比特）。如果 $q(x)$ 与 $p(x)$ 完全相同，交叉熵的值就等于真实分布的熵，即 $H(p, p) = H(p)$。

> [!NOTE]
> 
> 明白了，你是想了解为什么交叉熵损失函数采用 $y \log(\hat{y})$ 这种形式，它是如何从信息论的角度推导出来的。
> 
> ### 1. 信息论中的交叉熵
> 
> 交叉熵（Cross-Entropy）最初来源于信息论，用于衡量一个概率分布 $p(x)$ 与另一个概率分布 $q(x)$ 之间的差异。信息论中的交叉熵定义为：
> 
> $$
> H(p, q) = - \sum_{x} p(x) \log q(x)
> $$
> 
> 这里：
> - $p(x)$ 是真实分布（或称为真实标签的概率分布）。
> - $q(x)$ 是预测分布（或模型预测的概率分布）。
> 
> 交叉熵衡量了如果我们使用分布 $q(x)$ 来编码数据时，平均编码长度的增加程度与真实分布 $p(x)$ 的编码长度相比，越小的交叉熵意味着模型预测的分布越接近真实分布。
> 
> ### 2. 最小化交叉熵与最大化对数似然
> 
> 在机器学习和神经网络中，我们的目标通常是最小化损失函数，或者最大化似然函数。交叉熵损失实际上是最大化对数似然的负值。我们可以从最大化对数似然函数的角度来理解交叉熵损失。
> 
> 假设我们有一个分类任务，并且模型给出了预测的概率分布 $q(x)$，而真实标签是 $p(x)$。那么似然函数（Likelihood Function）可以表示为：
> 
> $$
> L(\theta) = \prod_{i} p(y_i | x_i)
> $$
> 
> 其中， $p(y_i | x_i)$ 是给定输入 $x_i$ 时，输出 $y_i$ 的概率。为了方便优化，我们通常取对数，得到对数似然函数（Log-Likelihood）：
> 
> $$
> \log L(\theta) = \sum_{i} \log p(y_i | x_i)
> $$
> 
> 接下来，我们希望最小化损失函数（交叉熵）：
> 
> $$
> \text{Loss} = - \log L(\theta) = - \sum_{i} \log p(y_i | x_i)
> $$
> 
> 这个式子就是交叉熵损失的核心公式。
> 
> ### 3. 损失函数的形式
> 
> 具体到每个样本 $i$，交叉熵损失函数的形式可以写成：
> 
> $$
> \text{Loss}(y_i, \hat{y}_i) = - \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
> $$
> 
> 这里：
> - $y_i$ 是真实标签，通常是一个二分类的值（0 或 1）。
> - $\hat{y}_i$ 是模型预测的概率。
> 
> ### 4. 为什么 $y \log(\hat{y})$？
> 
> - 当 $y = 1$ 时，我们希望 $\hat{y}$ 尽量接近 1，因此损失函数为 $-\log(\hat{y})$。
> - 当 $y = 0$ 时，我们希望 $\hat{y}$ 尽量接近 0，因此损失函数为 $-\log(1 - \hat{y})$。
> 
> **因此，交叉熵损失的目标是鼓励模型使得预测概率 $\hat{y}$ 与真实标签 $y$ 一致。**
> 
> ### 总结
> 
> 交叉熵损失函数的形式是由信息论中的交叉熵推导出来的，并与最大似然估计的目标相一致。最小化交叉熵等同于最大化对数似然，从而使得模型的预测概率分布尽可能接近真实标签的分布。因此，交叉熵损失函数在机器学习中是非常自然和合理的选择。

### 3. 交叉熵与 Kullback-Leibler 散度（KL Divergence）

在信息论中，交叉熵和 Kullback-Leibler 散度（KL 散度）有着紧密的关系。KL 散度是用来衡量两个概率分布之间的差异的一种非对称度量，它的定义是：

$$
D_{\text{KL}}(p \| q) = \sum_{i} p(x_i) \log \frac{p(x_i)}{q(x_i)}
$$

KL 散度度量的是当我们用分布 $q(x)$ 来近似分布 $p(x)$ 时，所增加的信息量或编码的“额外成本”。它总是大于或等于 0，因为它是基于熵的下界。

通过一些代数变换，可以将交叉熵与 KL 散度联系起来：

$$
H(p, q) = H(p) + D_{\text{KL}}(p \| q)
$$

这表明交叉熵是熵与 KL 散度的和。最小化交叉熵实际上是最小化 KL 散度，这意味着我们希望通过调整 $q(x)$ 来使其尽量接近 $p(x)$。

### 4. 为什么交叉熵是 $-p(x) \log q(x)$？

交叉熵中的 $-p(x) \log q(x)$ 可以从信息的角度理解。假设我们有一个真实的概率分布 $p(x)$，并且我们使用一个概率分布 $q(x)$ 来近似 $p(x)$ 进行编码。每次观察到一个事件 $x_i$，我们需要传递的信息量是 $-\log q(x_i)$，这就是我们从 $q(x)$ 生成该事件时所需的信息量。

但是，我们真正关心的是，当事件 $x_i$ 发生的概率是 $p(x_i)$ 时，使用 $q(x)$ 来编码这些事件需要的平均信息量。这就是交叉熵的含义：对所有事件加权求和，权重是事件的实际概率 $p(x_i)$，而每个事件的编码信息量是 $-\log q(x_i)$。

### 5. 交叉熵在机器学习中的应用

在机器学习中，尤其是在分类问题中，交叉熵损失函数用于度量模型输出的概率分布 $q(x)$ 与真实标签的概率分布 $p(x)$ 之间的差异。对于单个样本 $i$，真实标签 $y_i$ 通常是一个 one-hot 编码的概率分布，而模型的输出 $\hat{y}_i$ 是一个表示概率的向量。交叉熵损失函数就可以用来衡量这两个分布之间的差异，最小化损失即意味着最小化真实分布与预测分布之间的差异。

### 总结

信息论中的交叉熵公式 $- \sum_{i} p(x_i) \log q(x_i)$ 是基于熵的概念得出的，旨在度量两个概率分布之间的差异。它在机器学习中作为损失函数时，实际上是在最小化模型预测的概率分布与真实标签分布之间的差异，从而提高模型的准确性。

---

# 附录：二分类交叉熵

要理解为什么交叉熵损失函数的形式可以写成：

$$
\text{Loss}(y_i, \hat{y}_i) = - \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

我们需要从二分类问题和交叉熵的推导过程出发。

### 1. 二分类问题

在二分类问题中，假设模型的输出是一个概率 $\hat{y}_i$，表示样本属于正类（通常是类别 1）的概率。而真实标签 $y_i$ 是二分类问题中的标签，取值为 0 或 1：
- 当 $y_i = 1$，样本属于正类。
- 当 $y_i = 0$，样本属于负类。

我们使用模型的输出概率 $\hat{y}_i$ 来预测样本属于正类的概率，$1 - \hat{y}_i$ 则是样本属于负类的概率。

### 2. 交叉熵损失函数

对于单个样本 $i$，交叉熵损失函数衡量的是模型输出概率分布与真实标签概率分布之间的差异。假设真实标签的概率分布是 $p(y)$，而模型的预测概率分布是 $\hat{y}_i$，交叉熵的定义是：

$$
H(p, q) = - \sum_{i} p(y_i) \log q(y_i)
$$

在二分类中，真实标签的概率分布 $p(y_i)$ 只有两个可能的值：
- 当 $y_i = 1$ 时，真实标签的分布为 $p(y_i = 1) = 1$，其他 $p(y_i = 0) = 0$。
- 当 $y_i = 0$ 时，真实标签的分布为 $p(y_i = 0) = 1$，其他 $p(y_i = 1) = 0$。

所以，对于单个样本 $i$，交叉熵损失可以写成：

$$
H(p, q) = - \left[ p(y_i = 1) \log \hat{y}_i + p(y_i = 0) \log (1 - \hat{y}_i) \right]
$$

这里：
- $\hat{y}_i$ 是模型预测样本属于正类（$y_i = 1$）的概率。
- $1 - \hat{y}_i$ 是模型预测样本属于负类（$y_i = 0$）的概率。

### 3. 对于单个样本的交叉熵损失

为了更清楚地理解，我们将 $p(y_i = 1)$ 和 $p(y_i = 0)$ 与真实标签 $y_i$ 结合：

- 当 $y_i = 1$ 时，真实标签为正类，交叉熵损失为：

  $$
  \text{Loss}(y_i = 1, \hat{y}_i) = - \log(\hat{y}_i)
$$

- 当 $y_i = 0$ 时，真实标签为负类，交叉熵损失为：

  $$
  \text{Loss}(y_i = 0, \hat{y}_i) = - \log(1 - \hat{y}_i)
$$

### 4. 将两种情况合并

通过将这两种情况合并成一个统一的表达式，我们可以写成：

$$
\text{Loss}(y_i, \hat{y}_i) = - \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

这是因为：
- 当 $y_i = 1$ 时，公式中的第一个项 $y_i \log(\hat{y}_i) = \log(\hat{y}_i)$，第二项 $(1 - y_i) \log(1 - \hat{y}_i) = 0$。
- 当 $y_i = 0$ 时，公式中的第一个项 $y_i \log(\hat{y}_i) = 0$，第二项 $(1 - y_i) \log(1 - \hat{y}_i) = \log(1 - \hat{y}_i)$。

### 5. 结论

因此，这个公式是交叉熵损失函数在二分类问题中的一种简洁形式，它同时考虑了两个类别的情况：
- 如果真实标签是 1，那么损失是 $-\log(\hat{y}_i)$。
- 如果真实标签是 0，那么损失是 $-\log(1 - \hat{y}_i)$。

该形式既简洁又能有效衡量模型的预测概率与真实标签之间的差异。

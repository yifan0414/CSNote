---
created: 2025-04-01 20:41
tags:
---
随机梯度下降（Stochastic Gradient Descent, SGD）在实际应用中有多种变体，具体取决于每次参数更新时使用的样本数量。以下是三种主要形式的对比：

---

### 1. 随机梯度下降（SGD，单样本更新）
- **定义**：每次随机选取**一个样本**计算梯度并更新参数。
- **更新公式**：
  $$
  \mathbf{w} \leftarrow \mathbf{w} - \eta \nabla L(\mathbf{w}; \mathbf{x}_i, y_i)
$$
  - $\eta$：学习率  
  - $\nabla L(\mathbf{w}; \mathbf{x}_i, y_i)$：基于单个样本 $(\mathbf{x}_i, y_i)$ 的损失梯度。
- **特点**：
  - **计算快**：每次迭代仅需一个样本。
  - **噪声大**：梯度方向波动剧烈，可能震荡收敛。
  - **适合在线学习**：实时处理流式数据。

---

### **2. 小批量随机梯度下降（Mini-batch SGD）**
- **定义**：每次随机选取**一小批样本**（通常为 32、64、128 等）计算梯度。
- **更新公式**：
  $$
  \mathbf{w} \leftarrow \mathbf{w} - \eta \cdot \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla L(\mathbf{w}; \mathbf{x}_i, y_i)
$$
  - $\mathcal{B}$：小批量样本集合。
- **特点**：
  - **平衡效率与稳定性**：比单样本噪声小，比全批量计算快。
  - **硬件友好**：适合 GPU 并行计算，是深度学习中的默认选择。

---

### **3. 批量梯度下降（Batch GD，全数据更新）**
- **定义**：每次使用**全部训练数据**计算梯度。
- **更新公式**：
  $$
  \mathbf{w} \leftarrow \mathbf{w} - \eta \cdot \frac{1}{n} \sum_{i=1}^n \nabla L(\mathbf{w}; \mathbf{x}_i, y_i)
$$
- **特点**：
  - **梯度准确**：方向稳定，收敛路径平滑。
  - **计算成本高**：尤其数据量大时，每次迭代需遍历全数据集。
  - **内存限制**：无法处理超大规模数据（如数百万样本）。

---

### **对比总结**
| **方法**               | 每次更新样本数 | 噪声水平 | 计算速度 | 适用场景                     |
|------------------------|----------------|----------|----------|------------------------------|
| **单样本 SGD**         | 1              | 高       | 最快     | 在线学习、实时更新           |
| **小批量 SGD**         | $k$（如 32）  | 中       | 快       | 深度学习默认选择             |
| **批量 GD**           | 全部数据 $n$ | 低       | 慢       | 小规模数据或精确优化         |

---

### **常见问题解答**
#### **Q: 为什么深度学习常用小批量 SGD？**
- **效率**：GPU 可并行计算小批量的梯度，比单样本更充分利用硬件。
- **稳定性**：比单样本噪声低，比全批量内存需求小。

#### **Q: 小批量大小如何选择？**
- **一般规则**：32、64、128（2 的幂次，适配内存对齐）。
- **大批量**：可能收敛更快，但需调大学习率。
- **小批量**：更多噪声可能帮助逃离局部极小值。

#### **Q: 单样本 SGD 还有用吗？**
- 在**在线学习**或**强化学习**中仍有应用，但深度学习较少直接使用。

---

### **代码示例（PyTorch 小批量 SGD）**
```python
import torch
from torch.utils.data import DataLoader

# 数据加载（自动分小批量）
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# 优化器
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for inputs, targets in dataloader:
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()  # 小批量更新
```

---

### **总结**
- **严格意义上的“随机梯度下降”（SGD）指单样本更新**，但实际中“SGD”常泛指小批量版本。
- **小批量 SGD** 是深度学习的标准实践，平衡了速度与稳定性。
- 全批量 GD 仅适用于小数据或传统机器学习（如线性回归解析解不可用时）。
---
created: 2025-04-01 16:14
tags: 
link: https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities
---
这个公式描述了小批量随机梯度下降（Mini-batch Stochastic Gradient Descent, SGD）中的权重和偏置更新过程。让我们一步步地解析这个公式。

假设我们有一个训练集，其中每个样本 $i$ 包括输入特征 $\mathbf{x}^{(i)}$ 和目标标签 $y^{(i)}$，目标是通过训练模型来最小化某种损失函数。具体来说，模型是线性的，具有权重 $\mathbf{w}$ 和偏置 $b$，其预测值为 $\mathbf{w}^\top \mathbf{x}^{(i)} + b$。

### 1. 权重更新公式

$$
\mathbf{w} \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b)
$$

- $\mathbf{w}$ 是模型的权重向量。
- $\eta$ 是学习率，控制每次更新的步长。
- $\mathcal{B}$ 是当前小批量数据集的索引集合，$|\mathcal{B}|$ 表示批量大小，即样本数。
- $l^{(i)}(\mathbf{w}, b)$ 是第 $i$ 个样本的损失函数，通常是预测值与实际值之间的误差。为了简单起见，假设这里使用的是均方误差损失函数（或其他相关损失函数）。
- $\partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b)$ 是第 $i$ 个样本的损失对权重 $\mathbf{w}$ 的梯度。

**梯度计算：**

- 对于一个样本 $i$，其损失函数 $l^{(i)}$ 对权重 $\mathbf{w}$ 的梯度为：  
  $$
  \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{x}^{(i)} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)
$$
  这表示了预测误差（即 $\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}$）乘以输入特征 $\mathbf{x}^{(i)}$，这是对线性回归或类似模型的常见梯度。

> [!NOTE]
> 
> 为了理解如何得出梯度公式
> $$
> \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{x}^{(i)} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right),
> $$
> 我们首先需要定义损失函数，并通过求导的过程来得到这个结果。
> 
> 假设我们使用的是 **均方误差**（Mean Squared Error, MSE）作为损失函数，对于单个样本 $i$，损失函数可以写作：
> $$
> l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)^2.
> $$
> 
> ### 1. 损失函数形式
> 
> 在这里，$\mathbf{w}^\top \mathbf{x}^{(i)} + b$ 是模型的预测值，$y^{(i)}$ 是实际值。我们使用均方误差损失函数的常见做法，即对预测误差的平方进行最小化。
> 
> 为了方便求导，我们在损失函数前面加了一个 $\dfrac{1}{2}$，这个系数可以在求导时消去，简化计算。
> 
> ### 2. 对权重 $\mathbf{w}$ 求导
> 
> 我们现在对损失函数 $l^{(i)}(\mathbf{w}, b)$ 关于权重 $\mathbf{w}$ 求偏导数。首先我们对损失函数求导：
> 
> $$
> \frac{\partial}{\partial \mathbf{w}} l^{(i)}(\mathbf{w}, b) = \frac{\partial}{\partial \mathbf{w}} \left( \frac{1}{2} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)^2 \right).
> $$
> 
> 应用链式法则，首先对平方项求导：
> 
> $$
> \frac{\partial}{\partial \mathbf{w}} \left( \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)^2 \right) = 2 \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right) \cdot \textcolor{blue} {\frac{\partial}{\partial \mathbf{w}} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b \right).}
> $$
> 
> 然后对 $\mathbf{w}^\top \mathbf{x}^{(i)} + b$ 关于 $\mathbf{w}$ 求导：
> 
> $$
> \frac{\partial}{\partial \mathbf{w}} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b \right) = \mathbf{x}^{(i)}.
> $$
> 
> 因此，偏导数变为：
> 
> $$
> \frac{\partial}{\partial \mathbf{w}} l^{(i)}(\mathbf{w}, b) = \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right) \cdot \mathbf{x}^{(i)}.
> $$
> 
> ### 3. 结果解释
> 
> 这个结果表达了损失函数相对于 $\mathbf{w}$ 的梯度。具体来说，梯度是损失对预测误差（即 $\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}$）的导数，乘以输入特征 $\mathbf{x}^{(i)}$。这个公式的直观解释是，梯度的方向与特征 $\mathbf{x}^{(i)}$ 的方向有关，而梯度的大小则由预测误差的大小决定。
> 
> ### 结论
> 
> 所以，梯度公式  
> $$
> \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{x}^{(i)} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)
> $$
> 是通过对均方误差损失函数求导得到的，表示了损失函数关于模型权重 $\mathbf{w}$ 的梯度。

**权重更新：**

- 更新的过程就是沿着负梯度方向调整权重，减少损失函数。具体来说，权重更新是基于小批量中的样本平均梯度，表示通过计算当前小批量数据集中的梯度信息来调整权重。

### 2. 偏置更新公式

$$
b \leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)
$$

- $b$ 是模型的偏置。
- $\partial_b l^{(i)}(\mathbf{w}, b)$ 是第 $i$ 个样本的损失对偏置 $b$ 的梯度。

**偏置的梯度计算：**

- 对于每个样本，损失对偏置的梯度是：
  $$
  \partial_b l^{(i)}(\mathbf{w}, b) = \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}
$$
  这个梯度表示了预测误差（即模型输出与真实标签之间的差异）。

**偏置更新：**

- 偏置的更新类似于权重的更新，也沿着梯度的负方向调整。

### 总结

这个公式的核心是通过小批量数据集（batch）来估计梯度，然后基于这些梯度更新模型参数（包括权重和偏置）。每次更新时，权重和偏置都会朝着最小化损失函数的方向调整。通过计算小批量的平均梯度，而不是单个样本的梯度，可以减少梯度估计的噪声，提高训练效率。


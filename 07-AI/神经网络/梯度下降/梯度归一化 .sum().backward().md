---
created: 2025-04-01 20:33
tags:
---
```python showLineNumbers {11}
def sgd(params, lr, batch_size):  #@save
    """小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_() # 不会跨batch累积梯度
						
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # X和y的小批量损失
        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
        # 并以此计算关于[w,b]的梯度
        l.sum().backward() # 会计算当前 batch 内所有样本的梯度之和
        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```
是的，**即使小批量中所有样本的损失都被加总**，反向传播依然是正确的。我们可以从**梯度计算**的角度来解释为什么这样做不会出错。

### 1. **损失函数和反向传播的本质**

在反向传播中，计算的是损失函数相对于模型参数（例如权重 $w$ 和偏置 $b$）的梯度。假设我们使用的是 **批量损失**（Batch Loss），即针对整个小批量的损失，而不是单个样本的损失。

假设我们有一个小批量数据，其中包含 $N$ 个样本，对于每个样本 $i$，损失是 $l_i = \text{loss}(x_i, y_i)$（损失函数是样本 $i$ 的预测值与实际标签之间的差异）。如果我们将所有样本的损失加在一起，总损失就变成了：

$$
L_{\text{total}} = \sum_{i=1}^{N} l_i.
$$

这个总损失 $L_{\text{total}}$ 是一个标量，表示该小批量的所有样本损失的总和。

### 2. **梯度的计算**

当我们进行反向传播时，我们计算的是 **总损失** 相对于模型参数的梯度。公式为：

$$
\frac{\partial L_{\text{total}}}{\partial \theta} = \frac{\partial}{\partial \theta} \left( \sum_{i=1}^{N} l_i \right).
$$

由于 **梯度是线性的**，我们可以对每个样本的损失分别求梯度，并将结果相加：

$$
\frac{\partial L_{\text{total}}}{\partial \theta} = \sum_{i=1}^{N} \frac{\partial l_i}{\partial \theta}.
$$

> [!code]
> 
> ```python 
> import torch
> 
> # 确保 w 需要梯度
> w = torch.tensor(1.0, requires_grad=True)
> 
> # 批量 1
> batch_size = 10
> l = w * torch.ones(batch_size)  # 让 w 参与计算
> l.sum().backward()
> print(f'Batch 1: {w.grad}')  # 结果应该是 10
> 
> # 批量 2，不清空梯度
> batch_size = 20
> l = w * torch.ones(batch_size)
> l.sum().backward()
> print(f'Batch 2: {w.grad}')  # 结果应该是 10 + 20 = 30
> 
> # *只有在 w.grad 不是 None 时才能清零*
> if w.grad is not None:
>     w.grad.zero_()
> 
> # 批量 3
> batch_size = 5
> l = w * torch.ones(batch_size)
> l.sum().backward()
> print(f'Batch 3: {w.grad}')  # 结果应该是 5
> 
> """
> Batch 1: 10.0 
> Batch 2: 30.0 
> Batch 3: 5.0
> """
> ```
> 

### 3. **为什么 `.sum()` 是正确的？**

`l.sum().backward()` 的背后实际在做的就是将 **批量中的所有样本的梯度** 进行累加。这与我们计算总损失的方式是一致的：每个样本的损失对模型参数的梯度是单独计算的，然后将它们相加得到总的梯度。

因此，**通过 `.sum()` 来计算总损失是完全合理的**，因为它就是在模拟实际情况中的总损失，从而正确地计算梯度。`l.sum()` 是批量梯度计算中的一种常见做法，保证了反向传播能够正确进行。

### 4. **影响学习的方式**

需要注意的是，使用 `.sum()` 来计算损失的一个副作用是 **梯度的大小与批量大小有关**。假设批量大小为 $N$，那么每个样本的损失对梯度的贡献是相同的。因此，总损失的梯度是所有样本的梯度之和，**这意味着梯度的绝对值会随着批量大小的增大而增大**。

为了避免梯度大小随批量大小变化，通常会对损失函数进行归一化，**例如使用平均损失**而非总损失：

$$
L_{\text{avg}} = \frac{1}{N} \sum_{i=1}^{N} l_i.
$$

这样，梯度会是平均损失的梯度，不会受到批量大小的影响。具体来说，反向传播中的梯度将变为：

$$
\frac{\partial L_{\text{avg}}}{\partial \theta} = \frac{1}{N} \sum_{i=1}^{N} \frac{\partial l_i}{\partial \theta}.
$$

这样做的好处是 **保证梯度的规模保持一致**，不管批量大小如何。

### 5. **总结**

- **`.sum()`** 是对小批量中的所有样本损失进行求和，这样做是为了得到一个标量损失，从而正确计算梯度。
- 通过 **反向传播** 计算的总梯度是 **每个样本梯度的累加**，这是正确且一致的。
- 如果你不希望梯度受批量大小的影响，可以使用 **平均损失**，即每个样本的损失除以批量大小。

总之，`l.sum().backward()` 是一种常见且有效的计算梯度的方式，确保了梯度的正确计算和反向传播。
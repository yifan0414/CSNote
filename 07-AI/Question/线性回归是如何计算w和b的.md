---
created: 2025-03-29 22:30
tags:
---
## **1. 线性回归的数学表达**
假设数据集有 **m 个样本**，每个样本有 **n 个特征**：
- 设输入矩阵 $X$ 为 $m \times n$ 维
- 设参数 $w$ 为 $n \times 1$ 维
- 设偏置 $b$ 为标量

线性回归模型的预测值可以表示为：
$$
\hat{y} = X w + b
$$
目标是找到最优的 $w$ 和 $b$ 使得预测值 $\hat{y}$ 尽可能接近真实值 $y$。

### 损失函数（均方误差, MSE）
使用 **均方误差（Mean Squared Error, MSE）** 作为损失函数：
$$
L(w, b) = \frac{1}{2m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
$$
其中，$\frac{1}{2}$ 主要是为了求导时方便。

---

## **2. 方法 1：正规方程（Normal Equation）**
### **求解解析解**
目标是找到使损失函数最小化的参数，令损失函数对 $w$ 和 $b$ 的导数为 0：
$$
\frac{\partial L}{\partial w} = 0
$$
$$
\frac{\partial L}{\partial b} = 0
$$
通过求导计算，最优解为：
$$
w^* = (X^T X)^{-1} X^T y
$$

如果把偏置 $b$ 也包含在参数向量 $w$ 中，可以将输入矩阵 $X$ 扩展一列全 1（称为 **增广矩阵**），然后用相同的公式计算。

**优缺点**
✅ **优点**：
- 直接求解，无需迭代
- 计算精确，没有学习率调整的问题

❌ **缺点**：
- **时间复杂度高**：求逆矩阵 $(X^T X)^{-1}$ 需要 $O(n^3)$，对于高维数据计算量大
- 可能存在**矩阵不可逆**的问题，需要使用**伪逆** $(X^T X)^{+}$

---

## **3. 方法 2：梯度下降（Gradient Descent）**
对于大规模数据集，正规方程计算开销过大，通常使用**梯度下降**来求解。

### **(1) 计算梯度**
对损失函数关于 $w$ 和 $b$ 求偏导：
$$
\frac{\partial L}{\partial w} = \frac{1}{m} X^T (Xw + b - y)
$$
$$
\frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (Xw + b - y)
$$

### **(2) 参数更新**
使用**梯度下降**算法进行更新：
$$
w := w - \alpha \frac{\partial L}{\partial w}
$$
$$
b := b - \alpha \frac{\partial L}{\partial b}
$$
其中，$\alpha$ 是 **学习率**，决定更新步长大小。

### **(3) 迭代优化**
- **随机梯度下降（SGD）**：每次仅使用一个样本更新参数
- **小批量梯度下降（Mini-Batch GD）**：每次使用一个小批量数据
- **全批量梯度下降（Batch GD）**：每次使用全部数据计算梯度

**优缺点**
✅ **优点**：
- 适用于大规模数据集（不需要计算矩阵逆）
- 可用于非线性模型（如神经网络）

❌ **缺点**：
- 需要选择**合适的学习率**
- 可能会收敛到局部最优（但对于线性回归，MSE 是凸函数，不存在局部最优问题）

---

## **4. 选择哪种方法？**
| 方法 | 适用场景 | 计算复杂度 | 优势 | 劣势 |
|------|--------|--------|------|------|
| **正规方程** | 维度 $n$ 小，样本 $m$ 适中 | $O(n^3)$ | 无需调参，直接求解 | 数据维度太大时求逆矩阵计算昂贵 |
| **梯度下降** | 维度 $n$ 大，数据量 $m$ 大 | 迭代计算，线性复杂度 | 适用于大规模数据，可用于神经网络 | 需要选择合适的学习率，可能收敛较慢 |

---

## **5. 总结**
1. **正规方程**：
   - 直接求解析解，计算公式：
     $$
     w^* = (X^T X)^{-1} X^T y
     $$
   - 适用于小规模数据集，计算精确但不适合高维数据。
2. **梯度下降**：
   - 通过计算梯度，不断迭代更新：
     $$
     w := w - \alpha \frac{\partial L}{\partial w}
     $$
   - 适用于大规模数据集，但需要调整学习率。

在深度学习等大规模机器学习任务中，通常采用 **梯度下降及其变种（如 Adam, RMSprop）** 进行优化。
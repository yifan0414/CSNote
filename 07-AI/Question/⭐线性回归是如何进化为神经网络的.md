---
created: 2025-03-29 21:19
tags:
---
从**线性神经网络**（Linear Neural Network, LNN）到**多层感知机**（Multilayer Perceptron, MLP）的演化，是深度学习历史上的一个重要发展过程。我们可以从几个方面来探讨这一演化过程，包括网络结构、非线性引入、模型表达能力等。

### 1. 线性神经网络的基本构成

线性神经网络通常指的是具有单一层或浅层结构的神经网络，其中每个神经元执行的是线性变换。具体来说，网络的每一层都是对输入的线性组合，并通过加权求和后得到输出。假设网络只有一层（没有激活函数），其数学形式为：

$$
y = W \cdot x + b
$$

这里，**x** 是输入，**W** 是权重矩阵，**b** 是偏置，**y** 是输出。

**线性神经网络的局限性**：
- **线性变换有限**：如果每一层仅进行线性变换（即没有激活函数），则即使网络有多个层，整个网络的表达能力也和单层线性模型是一样的。因为多个线性变换的组合本质上仍然是一个线性变换。
- **无法建模非线性问题**：许多复杂的问题（例如图像识别、语音识别等）本质上是非线性的，线性神经网络无法有效处理这些问题。

### 2. 引入激活函数：解决非线性问题

为了让神经网络能够处理更复杂的非线性问题，引入了**非线性激活函数**（例如ReLU、Sigmoid、Tanh等）。通过在每一层的输出后应用一个非线性函数，神经网络就能够模拟复杂的非线性映射。

> [!note] 如果 **多层感知机（MLP）** 中没有激活函数，它将无法有效地解决非线性问题。 
> ### 为什么激活函数很重要？
> 
> 1. **线性变换的限制**：
>     
>     - 如果一个神经网络的所有层都只是进行**线性变换**（例如，仅执行加权和加上偏置的操作），那么无论网络有多少层，它的整体输出依然是线性的。换句话说，多个线性变换的组合本质上仍然是一个线性变换。
>         
>     - 比如，假设你的网络包含两层，每层都只是进行线性变换，那么整体输出依然可以通过一个等效的单层线性变换来表示。因为线性变换的组合仍然是线性操作，因此它无法捕捉数据中的非线性关系。
>         
> 2. **引入非线性的激活函数**：
>     
>     - 激活函数的作用是引入**非线性**，使得神经网络能够学习和逼近复杂的非线性函数。如果没有激活函数，网络就只能执行线性变换，即使增加了隐藏层，也无法解决任何非线性问题。
>         
>     - 常见的激活函数有 **ReLU**（Rectified Linear Unit）、**Sigmoid**、**Tanh** 等，这些函数能够对每个神经元的输出进行非线性变换，从而让神经网络能够学习更复杂的模式和关系。
>         
> 
> ### 举个例子：
> 
> 假设你有一个简单的神经网络，它的输入是二维数据（比如图像的像素），并且网络没有激活函数。如果你希望这个网络能够分类不同类型的图像（如猫和狗），这个网络实际上只是在寻找一个“分割超平面”来将不同的类别分开。但由于所有的层都没有激活函数，网络只能做线性分割，这意味着它无法有效处理复杂的、非线性可分的数据。
> 
> 一旦你加入了非线性激活函数，网络能够在更高维度空间中找到更复杂的决策边界，这样它就能够处理更加复杂的分类任务，比如图像识别、语音识别等。
> 
> ### 总结：
> 
> 如果 **多层感知机（MLP）** 中没有激活函数，那么无论网络有多少层，它仍然只是一个简单的线性变换。因此，**没有激活函数的网络无法处理非线性问题**，它的能力就像一个简单的线性回归模型，无法学习和拟合复杂的非线性数据。因此，激活函数对于神经网络来说是必不可少的，它是实现非线性建模的关键。


以 **Sigmoid 激活函数** 为例，它的公式为：

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

在一个层中，线性变换后的输出 $z = W \cdot x + b$ 会通过激活函数，变成：

$$
y = \sigma(W \cdot x + b)
$$

通过引入激活函数，神经网络的每一层不再只是简单的线性变换，而是能够引入非线性因素，从而增加了网络的表达能力。

### 3. 从单层到多层：多层感知机的引入

一个单层的神经网络（例如感知机）能够处理一些简单的分类问题，但对于更复杂的任务，单层网络往往无法捕捉到数据中的复杂模式。这时，引入多层结构（即多层感知机，MLP）成为了解决这一问题的关键。

**多层感知机（MLP）** 是一种前馈神经网络，它由多个层组成，每一层都包含若干神经元，其中每个神经元都是一个包含非线性激活函数的计算单元。MLP的基本结构如下：

- **输入层**：接收原始数据。
- **隐藏层**：一层或多层神经元，进行非线性变换。
- **输出层**：生成最终的结果。

在MLP中，信号从输入层经过隐藏层传递到输出层。每一层的输出都是基于前一层的加权和经过激活函数的处理。这样，网络的每一层都能够捕捉不同层次的数据特征，从而增强了网络对复杂问题的建模能力。

**MLP的优势**：
- **高表达能力**：多层结构使得神经网络可以表达复杂的非线性关系。
- **拟合任意函数**：根据**通用逼近定理**，一个包含足够多隐藏层的神经网络可以逼近任何连续函数，因此它具有非常强的拟合能力。

### 4. 梯度下降和反向传播

在早期的神经网络中，训练多层网络非常困难，因为缺乏有效的优化方法。直到1986年，**反向传播算法**的提出才使得训练多层神经网络成为可能。反向传播允许通过梯度下降更新每一层的权重，使得网络能够逐步减小预测误差。

反向传播的基本思路是：
- 从输出层开始，计算误差，并通过链式法则将误差反向传播到每一层。
- 使用梯度下降方法更新每一层的权重。

### 总结：从线性神经网络到多层感知机的演化

- **线性神经网络** 只能够处理简单的线性问题，且网络深度增加也无法提高其表达能力。
- 引入 **激活函数**（例如Sigmoid、ReLU等）后，网络具备了表达非线性关系的能力，解决了线性神经网络的局限。
- **多层感知机**（MLP）通过堆叠多个带激活函数的层，能够捕捉更加复杂的数据模式，提升了神经网络的表现。
- **反向传播算法** 解决了如何有效训练多层神经网络的问题，使得多层感知机成为了深度学习的重要基础。

这个演化过程标志着神经网络从一个简单的线性模型，逐渐发展成能够处理复杂任务的多层、非线性模型，开创了现代深度学习的时代。

**从线性回归到神经网络：一个逐步推导**

神经网络是从最基础的线性回归和逻辑回归逐步发展而来的，我们可以从最简单的线性回归出发，一步步引入新的概念，最终构造出神经网络。

---

**1. 线性回归（Linear Regression）**

**(1) 定义**

线性回归用于拟合输入 $x$ 和输出 $y$ 之间的线性关系：
$$
y = w x + b
$$
其中：
-  $w$ 是权重（Weight），决定输入的影响程度。
-  $b$ 是偏置（Bias），用来调整整体偏移量。

如果是多维输入（多个特征），则可以写成：

$$
y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
$$

或者使用矩阵表示：
$$
\hat{y} = X W + b
$$
其中：

-  $X$ 是输入特征矩阵（$m \times n$）。
- $W$ 是权重向量（$n \times 1$）。
- $b$ 是偏置项。

**(2) 目标**

使用 **均方误差（MSE）** 作为损失函数：

$$
L = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_i - y_i)^2
$$

利用 **梯度下降（Gradient Descent）** 来优化 $w$ 和 $b$ 使得损失最小。

---

**2. 逻辑回归（Logistic Regression）**

**(1) 为什么需要逻辑回归？**

• 线性回归适用于连续值预测，但在分类问题（如 $y = 0$ 或 $y = 1$）时，可能会出现预测值超出 $[0,1]$ 的情况。

• 解决方案：将线性回归的输出转换为 **概率值**（$0 \sim 1$）。

  

**(2) 使用 Sigmoid 作为激活函数**

逻辑回归的核心改进是：

$$
p = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

其中 $z = w x + b$，$\sigma(z)$（Sigmoid 函数）将结果压缩到 $(0,1)$ 之间。
  

**(3) 目标**

  

使用 **交叉熵损失（Binary Cross Entropy）**：
$$
L = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right]
$$

同样使用 **梯度下降** 进行优化。

---

**3. 进一步推广到神经网络**

逻辑回归仍然是一个**线性模型**，但现实世界的数据往往具有 **非线性关系**。如何让模型更强大？——**增加隐藏层（Hidden Layer），引入神经网络！**

**(1) 单层神经网络（感知机）**

**结构**

• **输入层（Input Layer）**：$X$

• **权重（Weights）**：$W$

• **偏置（Bias）**：$b$

• **激活函数（Activation Function）**：$\sigma(x)$

• **输出层（Output Layer）**：$\hat{y}$

计算公式：

$\hat{y} = \sigma(WX + b)$

这和逻辑回归几乎一样，但它已经是 **神经网络的一个基础单元（感知机，Perceptron）** 了。

---

**(2) 多层神经网络（MLP）**

  

单层神经网络仍然是**线性模型**，为了让它具备更强的非线性表达能力，我们可以 **增加隐藏层**：

$Z^{(1)} = W^{(1)}X + b^{(1)}$

$A^{(1)} = \sigma(Z^{(1)})$

$Z^{(2)} = W^{(2)}A^{(1)} + b^{(2)}$

$A^{(2)} = \sigma(Z^{(2)})$

  

这里的 $A^{(1)}$ 就是**隐藏层的输出**，它相当于对输入进行了**非线性变换**，然后再传递到下一层进行计算。多个隐藏层组合起来，就构成了深度神经网络（DNN）。

---

**(3) 引入更多激活函数**

  

使用 **Sigmoid** 作为激活函数会有梯度消失问题，因此神经网络引入了 **ReLU**、**Leaky ReLU** 等更好的激活函数：

  

$\text{ReLU}(x) = \max(0, x)$

  

ReLU 可以解决梯度消失问题，提高训练速度。

---

**(4) 反向传播（Backpropagation）**

• 多层神经网络的参数更新使用 **反向传播（BP, Backpropagation）**，其核心是 **链式法则**：
$$
\frac{\partial L}{\partial W} = \frac{\partial L}{\partial A} \cdot \frac{\partial A}{\partial W}
$$
• 通过 **梯度下降** 或 **Adam** 优化器更新参数。

---

**4. 总结**

|**过程**|**主要公式**|**关键改进**|
|---|---|---|
|**线性回归**|$y = Wx + b$|仅能拟合线性关系|
|**逻辑回归**|$\hat{y} = \sigma(Wx + b)$|引入 Sigmoid 解决二分类问题|
|**单层神经网络（感知机）**|$\hat{y} = \sigma(WX + b)$|本质上仍是逻辑回归|
|**多层神经网络（MLP）**|$A^{(l)} = \sigma(W^{(l)}A^{(l-1)} + b^{(l)})$|引入隐藏层，增加非线性表达能力|
|**深度神经网络（DNN）**|多层神经元组合|采用 ReLU、反向传播等优化|

**核心思路**：

1. **从线性到非线性**：线性回归 → 逻辑回归（加上 Sigmoid） → 多层感知机（加隐藏层）。

2. **引入非线性变换**：通过激活函数（如 ReLU）增强学习能力。

3. **深度学习**：增加隐藏层并使用反向传播优化。

  

最终，神经网络发展出了 CNN、RNN、Transformer 等更强大的结构，实现了图像识别、语音识别、自然语言处理等多种应用！ 🚀
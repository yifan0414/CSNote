{
	"nodes":[
		{"id":"cb078263bcfcfbd5","x":-720,"y":-280,"width":712,"height":642,"type":"text","text":"# LLaVA\n\n\n> [!quote]  **“一句话”**：*LLaVA 用 GPT-4 写“指南”，让 CLIP+LLaMA 在 158K 造的数据上学会看图说话，8 块 A100 就能炼出类 GPT-4 的多模态 ChatBot。*  \n\n**论文基本信息**  \n- **题目**：*LLaVA: Large Language and Vision Assistant — Visual Instruction Tuning*  \n- **作者**：Haotian Liu\\*, Chunyuan Li\\*, Qingyang Wu, Yong Jae Lee（\\*共同一作）  \n- **收录**：NeurIPS 2023 Oral  \n- **arXiv**：2304.08485，首次提交 2023-04-17，最新修订 2023-12-11  ([Visual Instruction Tuning](https://arxiv.org/abs/2304.08485?utm_source=chatgpt.com))  \n\n---\n\n### 1 研究动机  \n文本领域的 *instruction tuning* 让纯语言 LLM 在零样本任务上能力大涨；然而在「图文混合」场景还鲜有类似方法。作者提出 **Visual Instruction Tuning (VIT)**：先用强大的语言模型 GPT-4 自动生成 *图像-文本指令-响应* 三元组，再用这些数据去微调多模态模型，从而低成本获得类 GPT-4 的视觉问答／聊天能力。 ([Visual Instruction Tuning](https://arxiv.org/abs/2304.08485?utm_source=chatgpt.com), [Visual Instruction Tuning | OpenReview](https://openreview.net/forum?id=w0H2xGHlkw&utm_source=chatgpt.com))  \n\n---\n\n### 2 方法框架  \n\n| 阶段 | 目标 | 数据 | 参数更新 | 说明 |\n|------|------|------|---------|------|\n| **Stage 1：视觉语义对齐** | 让语言模型能“听懂”CLIP图像特征 | **CC-595K** 过滤版图文对齐集 | 仅更新一个 **线性投影层** | 把 CLIP ViT-L/14 视觉编码映射到 LLM 词嵌入空间  |\n| **Stage 2：Visual Instruction Tuning** | 让模型学会遵循视觉指令对话 | **LLaVA-Instruct-158K**（GPT-4 生成：58K 对话、23K 详细描述、77K 复杂推理） | 冻结视觉编码器，仅微调投影层 + LLM | 共 8×A100，10 h 完成微调  ([[PDF] Visual Instruction Tuning](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf?utm_source=chatgpt.com)) |\n| **可选 Stage 3：任务微调** | 下游任务精调（如 ScienceQA） | 少量标注 | 同上 | 与 GPT-4 集成拿到 SOTA 92.53%  ([Visual Instruction Tuning](https://arxiv.org/abs/2304.08485?utm_source=chatgpt.com), [Visual Instruction Tuning | OpenReview](https://openreview.net/forum?id=w0H2xGHlkw&utm_source=chatgpt.com)) |\n\n> **模型结构**：`CLIP ViT-L/14` → `线性投影 (∼14 M)` → `Vicuna/LLaMA-7B or 13B` decoder，端到端自回归生成。  \n\n---\n\n### 3 实验结果与分析  \n\n| 任务 / 指标 | 成绩 | 对比 / 亮点 |\n|-------------|-------|-------------|\n| **LLaVA-Bench** (作者自建多模态指令评测) | *85.1 %* 相对 GPT-4 | 远超 BLIP-2、OpenFlamingo 等同规模模型  ([Visual Instruction Tuning](https://arxiv.org/abs/2304.08485?utm_source=chatgpt.com)) |\n| **ScienceQA (图片子集)** | **92.53 %** (LLaVA + GPT-4 ensemble) | 新 SOTA，验证视觉理解+外部知识协同  ([Visual Instruction Tuning](https://arxiv.org/abs/2304.08485?utm_source=chatgpt.com)) |\n| **定性对话示例** | 展现跨图像推理、代码生成、梗图解释等“涌现”能力 | 与 GPT-4 行为相似，偶有幻觉  |\n\n---\n\n### 4 贡献总结  \n1. **提出 VIT 框架**：首次用纯文本 GPT-4 自动打造高质量多模态指令数据集。  \n2. **构建 LLaVA 模型**：简单线性投影+少量 GPU 训练即可把开源 LLM 变成通用视觉助手。  \n3. **公开资源**：158K 指令数据、代码、模型权重与两套评测基准，为社区进一步研究提供起点。 ([LLaVA](https://llava-vl.github.io/?utm_source=chatgpt.com))  \n4. **实证效果**：零样本视觉任务中逼近 GPT-4，多任务表现优于同期开源多模态 LLM。  \n\n---\n\n### 5 局限与后续方向  \n- **单图限制**：当前只支持单张图片输入，多图推理与视频理解待探索。  \n- **视觉细节丢失**：线性投影简单高效，却可能无法捕获空间关系与 OCR 细节；后续已出现 LLaVA-1.5、LLaVAR 等改进。 ([LLaVAR](https://llavar.github.io/?utm_source=chatgpt.com))  \n- **幻觉 & 偏见**：继承 CLIP / LLaMA 的固有偏差，对安全性与事实性需额外约束与过滤策略。  \n- **算力可扩展性**：作者实验显示 8×A100 即可复现；但更大模型（65B）与更多数据（SVIT 4.2 M）可能带来显著提升。 ([[PDF] SVIT: Scaling up Visual Instruction Tuning - arXiv](https://arxiv.org/pdf/2307.04087?utm_source=chatgpt.com))  \n\n---\n\n### 6 与现有工作比较  \n| 方法               | 视觉编码器      | 语言模型         | 训练策略                           | 主要区别                              |\n| ---------------- | ---------- | ------------ | ------------------------------ | --------------------------------- |\n| **BLIP-2**       | ViT-G      | OPT / FlanT5 | 二阶段 Query Transformer + LLM 对齐 | 不做指令调教，侧重检索式推理                    |\n| **OpenFlamingo** | CLIP       | GPT-J        | Perceiver Resampler + 跨模态对齐    | requires 长序列 Few-Shot             |\n| **LLaVA**        | CLIP ViT-L | LLaMA/Vicuna | VIT + GPT-4 生成 158K 指令微调       | 首个系统性把 *instruction tuning* 引入多模态 |\n"}
	],
	"edges":[]
}